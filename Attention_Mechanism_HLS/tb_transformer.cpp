#include <stdio.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <iostream>

#include "transformer.h"

float emb_size = 24.0;
void score_matrix(float score_m[M][O], float queries[M][N], float keys_t[N][O]);
void softmax_function(float attention_hx[M][O], float score_m_hx_div[M][O], float sum_score_m_hx[M]);
void output_matrix(float output_hx[M][N], float attention_hx[M][O], float values[O][N]);
void output_matrix_linear(float concatenated_output_matrix_linear[M][P], float concatenated_total_output_matrix[M][Q], float matrix_weights_linear1_t[Q][P], float bias_linear[Q]);
void output_matrix_linear1_fw(float output_layer1_fw[M][K], float norm_layer1[M][Q], float matrix_weights_linear1_fw_t[Q][K], float bias_linear1_fw[K]);

// Function to check if the attention matrix is correct, return the Average
float attention_check(float total_attention_h_pytorch[M][O*8], float total_attention_h[M][O*8]){
	float sum;
	for(int m=0; m<M; m++){
		for(int o=0; o<O*8; o++){
			sum += total_attention_h_pytorch[m][o]-total_attention_h[m][o];
		}
	}
	return sum/(M*emb_size);
}

// Function to check if the output matrix is correct (after linear layer, output MHA layer), return the Average
float output_attention_check(float concatenated_output_matrix_linear[M][P], float concatenated_output_matrix_linear_pytorch[M][P]){
	float sum;
	for(int m=0; m<M; m++){
		for(int p=0; p<P; p++){
			sum += concatenated_output_matrix_linear_pytorch[m][p]-concatenated_output_matrix_linear[m][p];
		}
	}
	return sum/(M*emb_size);
}

int main(){
//head 1
	float Values1[M][N] = {0.8320,0.4655,-0.6038,-0.7189,0.7088,-1.1346,-0.3481,1.2694,-1.7224};
	float Keys1[M][N] = {0.4005,0.4842,-0.7993,0.1271,0.8800,-0.0910,0.6990,1.3242,-0.3452};
	float Keys_t1[N][M];
	float Queries1[M][N] = {-1.5804,-1.6719,-1.2230,-1.3261,-1.1407,0.0731,-2.6327,-2.3064,-0.6516};
//head2
	float Values2[M][N] = {0.4183,-0.3233,0.3265,0.1963,0.2410,0.2989,1.7226,-0.5240,1.1437};
	float Keys2[M][N] = {-0.2709,-0.2298,-0.3485,0.8951,-0.2975,0.7939,0.4011,-0.8921,-0.3115};
	float Keys_t2[N][M] = {};
	float Queries2[M][N] = {0.3109,0.0685,-0.2500,0.0862,0.5307,0.0033,0.5468,0.3401,-1.1022};
//head3
	float Values3[M][N] = {0.1482,0.0592,-0.3047,0.0791,0.3102,0.2522,1.2747,-0.0803,0.5511};
	float Keys3[M][N] = {-0.2107,0.2648,-0.5181,0.9769,-0.2696,0.9283,0.6185,-0.4456,-0.1715};
	float Keys_t3[N][M] = {};
	float Queries3[M][N] = {-0.5077,-0.6919,-0.3458,0.0667,0.5862,0.1006,-0.1413,-0.1486,-0.9833};
//head4
	float Values4[M][N] = {2.0118,-0.4005,0.7315,-0.6136,0.3192,-0.0803,-0.0619,0.2621,0.3233};
	float Keys4[M][N] = {0.2205,-0.5384,-1.0002,0.5270,0.0072,0.9031,0.9393,-0.3286,1.0669};
	float Keys_t4[N][M] = {};
	float Queries4[M][N] = {-0.2261,-0.6531,-1.6596,0.1241,0.5840,0.6114,0.2777,0.8261,0.2899};
//head5
	float Values5[M][N] = {0.9618,0.5331,-0.2549,-0.4450,0.5419,-0.7032,-0.8138,0.7292,-0.7440};
	float Keys5[M][N] = {0.9701,0.1675,-0.1855,0.3014,0.5296,0.1290,0.6171,0.5280,0.6762};
	float Keys_t5[N][M] = {};
	float Queries5[M][N] = {-1.2835,-1.0797,-1.1303,-0.8843,-0.6509,0.0518,-0.7978,-0.2878,0.4238};
//head6
	float Values6[M][N] = {-1.2435,0.2784,-1.2241,-0.9549,-0.1548,-0.0493,1.3784,-0.1197,0.3268};
	float Keys6[M][N] = {-0.9722,1.0129,-0.6286,-0.5045,0.0443,0.3524,0.2829,-0.2380,-0.6991};
	float Keys_t6[N][M] = {};
	float Queries6[M][N] = {-0.8204,-1.0705,0.5334,0.6792,0.6948,0.9545,-0.4612,-0.6988,-1.2380};
//head7
	float Values7[M][N] = {2.4266,-0.2220,1.8406,0.0230,-0.2076,0.8290,-1.4274,0.3231,-2.5179};
	float Keys7[M][N] = {1.9528,-1.5397,0.8837,0.5218,-0.7029,1.0055,-2.4094,2.1493,-2.5929};
	float Keys_t7[N][M] = {};
	float Queries7[M][N] = {0.7382,1.2077,-1.3698,1.1210,1.4656,0.4926,-2.3010,-3.3765,-0.1002};
//head8
	float Values8[M][N] = {0.2399,-0.1561,0.0662,0.2091,-0.2408,0.1470,-1.0604,-1.1814,0.3908};
	float Keys8[M][N] = {-0.2333,-0.0289,-0.3511,-0.3260,-0.0883,-0.3450,-2.2289,-0.1906,-0.6861};
	float Keys_t8[N][M] = {};
	float Queries8[M][N] = {0.0129,-0.1784,-0.2189,0.1834,-0.0408,-0.1409,1.9078,1.0742,1.2752};
// Parameters to store and print ALL Values, Keys and Queries (not for calculations, only for displaying)
	float total_Values[M][N*8] = {};
	float total_Keys[M][N*8] = {};
	float total_Keys_transpose[N][M*8] = {};
	float total_Queries[M][N*8] = {};
// Parameters to store the score matrix for each head
	float score_m_h1[M][O] = {};
	float score_m_h2[M][O] = {};
	float score_m_h3[M][O] = {};
	float score_m_h4[M][O] = {};
	float score_m_h5[M][O] = {};
	float score_m_h6[M][O] = {};
	float score_m_h7[M][O] = {};
	float score_m_h8[M][O] = {};
// Parameters to store and print ALL score m. for 8 heads
	float total_score_m[M][O*8] = {};
	float total_score_m_div[M][O*8] = {};
	float sqrt_emb_size;
	float score_m_h1_div[M][O] = {};
	float score_m_h2_div[M][O] = {};
	float score_m_h3_div[M][O] = {};
	float score_m_h4_div[M][O] = {};
	float score_m_h5_div[M][O] = {};
	float score_m_h6_div[M][O] = {};
	float score_m_h7_div[M][O] = {};
	float score_m_h8_div[M][O] = {};
// Parameters to store and print ALL attention for 8 heads
	float sum_score_m_h1[M] = {};
	float sum_score_m_h2[M] = {};
	float sum_score_m_h3[M] = {};
	float sum_score_m_h4[M] = {};
	float sum_score_m_h5[M] = {};
	float sum_score_m_h6[M] = {};
	float sum_score_m_h7[M] = {};
	float sum_score_m_h8[M] = {};
	float total_attention_h[M][O*8] = {};
	float attention_h1[M][O] = {};
	float attention_h2[M][O] = {};
	float attention_h3[M][O] = {};
	float attention_h4[M][O] = {};
	float attention_h5[M][O] = {};
	float attention_h6[M][O] = {};
	float attention_h7[M][O] = {};
	float attention_h8[M][O] = {};
// Parameters for the verification of the attention
	float total_attention_h_pytorch[M][O*8] = {};
	float attention_h1_pytorch[M][O] = {0.4152,0.3320,0.2528,0.3628,0.3601,0.2771,0.4141,0.3623,0.2236,};
	float attention_h2_pytorch[M][O] = {0.3282,0.3331,0.3387,0.3383,0.3431,0.3186,0.3451,0.3026,0.3523,};
	float attention_h3_pytorch[M][O] = {0.3508,0.3020,0.3472,0.3427,0.3366,0.3207,0.3752,0.2756,0.3492,};
	float attention_h4_pytorch[M][O] = {0.5133,0.2470,0.2397,0.2692,0.3671,0.3638,0.2883,0.3600,0.3516,};
	float attention_h5_pytorch[M][O] = {0.3504,0.3585,0.2910,0.3222,0.3476,0.3303,0.3087,0.3463,0.3449,};
	float attention_h6_pytorch[M][O] = {0.3004,0.3820,0.3176,0.3217,0.3622,0.3162,0.3403,0.2918,0.3679,};
	float attention_h7_pytorch[M][O] = {0.1866,0.1787,0.6347,0.3676,0.3445,0.2879,0.3633,0.3987,0.2381,};
	float attention_h8_pytorch[M][O] = {0.3314,0.3320,0.3366,0.3407,0.3396,0.3197,0.4237,0.4040,0.1723,};
// Parameters to store and print ALL outputs after Attention_hx * Values & Concatenation
	float concatenated_total_output_matrix[M][N*8] = {};
	float output_h1[M][N] = {};
	float output_h2[M][N] = {};
	float output_h3[M][N] = {};
	float output_h4[M][N] = {};
	float output_h5[M][N] = {};
	float output_h6[M][N] = {};
	float output_h7[M][N] = {};
	float output_h8[M][N] = {};
// Parameters to store and print the output of the linear layer
	float concatenated_output_matrix_linear[M][P] = {};
	float matrix_weights_linear1[P][Q] = {0.1271,-0.0144,0.1752,-0.0177,-0.1325,0.1114,0.1162,0.1220,0.0894,-0.0882,0.1755,-0.1109,-0.1477,-0.0937,-0.0155,-0.0756,0.1904,0.2029,0.0121,-0.1968,-0.0470,-0.1315,-0.1192,-0.2013,0.1703,0.0740,0.1115,-0.0892,0.1715,-0.1196,-0.0511,-0.1837,-0.1918,0.1770,0.0895,-0.0267,-0.0399,0.0381,-0.1673,-0.0468,-0.1739,-0.1750,0.1778,0.0973,0.0865,0.1019,-0.1546,-0.0320,0.0309,-0.0916,0.1907,-0.0986,0.0706,0.0565,0.0318,-0.1651,-0.0489,0.0706,-0.0355,-0.0103,0.0617,0.1701,-0.1482,-0.1372,0.0989,-0.0954,0.1712,-0.1966,0.1808,-0.1297,-0.1781,-0.0752,-0.1631,0.0855,-0.1207,0.1997,0.0800,0.0865,0.1846,0.0773,-0.0967,0.0847,0.0160,0.1658,-0.0945,0.1358,-0.1559,-0.1129,-0.0342,-0.1691,-0.0281,0.0906,-0.0098,0.1113,0.1756,0.0392,0.0650,-0.0237,-0.0714,0.0416,0.1073,-0.0044,0.0317,0.1600,-0.1040,0.1632,-0.0416,-0.0968,0.0270,-0.0347,-0.1375,0.1067,-0.1937,-0.0630,0.1493,0.1049,-0.0709,0.0778,-0.0256,-0.2005,0.0194,-0.1533,0.2036,0.0452,-0.0973,0.0387,0.1369,0.1517,-0.0874,-0.1364,-0.1751,-0.1117,0.0449,0.1663,0.0449,-0.0297,0.0154,0.0643,0.1993,0.1694,-0.1568,-0.1777,0.1545,-0.0110,-0.1515,-0.1333,0.0594,0.0011,0.1508,0.1830,-0.1613,0.0998,0.0502,0.1756,-0.0193,-0.0755,0.1280,0.0420,-0.0985,-0.1497,0.1080,-0.0951,-0.1676,0.0228,-0.1459,-0.0316,0.0450,-0.1781,0.0307,0.0930,-0.1119,0.0857,0.0538,-0.0072,0.1881,0.0408,-0.0131,0.1198,-0.1520,-0.1242,0.0323,-0.1178,-0.0981,0.1694,0.0360,0.1072,-0.1257,0.2037,0.1890,0.0345,-0.1852,-0.0855,-0.0665,-0.0210,-0.1846,0.1852,-0.1956,0.0275,-0.0974,0.1535,-0.1582,0.1775,-0.0856,0.1992,0.0492,-0.1372,0.0026,-0.1758,0.1416,0.0346,-0.0640,-0.0526,-0.2023,0.1106,0.1753,-0.1310,0.0460,0.1600,-0.1802,0.2019,0.1878,-0.1692,0.1319,-0.1563,-0.0901,0.1124,0.1683,-0.0079,0.1253,0.0056,0.1276,0.1667,-0.0710,-0.0679,0.1865,-0.1632,0.1581,-0.0283,-0.2032,0.0773,-0.1055,0.1334,-0.1145,-0.0700,0.0852,-0.0861,0.1444,-0.0473,-0.1526,-0.1273,0.1103,0.0426,0.0701,0.1545,0.0175,-0.1158,0.1774,0.1804,0.0387,-0.1612,0.1220,0.0825,0.0876,0.0233,0.1061,0.1656,0.0563,-0.1824,-0.0417,0.1855,-0.0580,-0.1944,-0.0584,0.0582,-0.1960,-0.1390,-0.0044,0.1445,0.0626,-0.1481,-0.0635,0.1090,-0.1303,-0.1654,-0.1797,0.0241,0.1957,0.0454,-0.1600,-0.0862,-0.1798,-0.1007,0.1786,0.0524,0.1988,0.1117,-0.0821,-0.0920,-0.1301,0.1083,-0.1387,0.1678,0.1526,0.1041,0.1482,-0.1789,-0.0954,-0.0363,0.0182,-0.1081,0.1753,-0.0998,0.0216,-0.1235,0.1967,-0.0007,0.1563,-0.0348,0.0316,-0.0910,0.0666,0.0430,0.1452,0.1344,-0.0317,-0.1114,0.0041,-0.1243,0.0367,-0.0113,-0.0216,0.0326,-0.0987,-0.0316,-0.1211,0.0680,-0.0629,0.0500,0.1267,0.0895,-0.0932,0.1955,-0.0192,0.0737,0.0371,0.1141,0.0890,0.0823,0.0195,-0.1248,-0.0896,-0.1866,-0.0911,-0.0137,0.0556,0.0694,0.2016,0.0388,0.1463,0.0767,0.1997,0.0094,-0.1303,-0.1951,0.1265,0.0133,-0.1014,0.1111,0.1748,0.1609,-0.1198,0.0346,-0.1639,0.1699,-0.1732,-0.0908,-0.0708,0.1588,-0.1829,0.0735,0.0208,-0.1005,-0.1997,0.1201,-0.0304,0.1895,-0.2015,0.0203,0.0677,-0.1150,-0.0991,0.1495,-0.0819,-0.0664,0.0325,0.1527,-0.1491,0.0105,0.1859,0.1163,0.1157,0.0491,0.0236,-0.1540,0.0613,0.1385,-0.0965,-0.1766,0.0760,-0.0658,0.0116,-0.0203,-0.0616,0.0763,-0.1248,-0.0771,0.0530,-0.0257,-0.0763,-0.1117,0.0431,-0.0758,0.0480,0.1344,-0.1318,0.1447,0.0648,0.1068,0.1268,-0.1868,-0.0504,0.0179,0.0736,0.0576,0.0931,-0.1821,0.0623,0.1449,0.0088,-0.0574,0.1622,-0.0657,0.0151,0.0951,-0.1942,-0.1180,-0.0546,-0.0501,0.1297,-0.0451,0.1353,-0.2013,0.0595,0.0047,0.1360,0.0789,0.1811,-0.0161,-0.1120,-0.1266,-0.1117,-0.1454,-0.1636,-0.0841,0.2000,0.0882,-0.1883,0.0212,0.0040,0.0634,0.0157,0.1246,-0.0817,0.1671,0.0924,0.0153,-0.1038,-0.1293,-0.0811,0.0964,0.1589,0.1557,0.1663,0.1982,-0.0937,0.0459,0.0889,-0.1552,-0.0946,0.1579,0.1569,0.0089,-0.1171,-0.1423,-0.0803,-0.0692,-0.1140,0.1200,-0.1953,0.1767,0.1296,-0.1066,-0.0468,0.1770,-0.0594,-0.1341,0.1405,0.0685,0.1410,0.0549,-0.1995,-0.1015,-0.2040,0.1649,-0.1668,-0.2002,-0.0916,0.1163,-0.0106,-0.1278,-0.0174,0.0245,0.0942,0.0493,0.1220,0.1286,0.1001,0.0807,0.1424,0.0821,-0.1441,-0.1179,-0.1338,-0.1332,-0.0872,0.1145,0.1172,0.1163,0.1575,-0.1734,0.1782,-0.2013,0.1017,-0.0791,-0.1094,0.0381,-0.0181,-0.0814,-0.0310,0.0731,-0.1365,-0.1234,-0.1393,0.1758,0.1386,0.0666,0.0354,-0.1466,-0.1895,-0.1303,-0.0240,0.1292,-0.0166,-0.1235,0.0814,-0.0197,-0.0745,0.1050,0.1450,0.1459,0.1961,0.0610,-0.1438,-0.0127,-0.1002,0.1996};
	float matrix_weights_linear1_t[Q][P] = {};
	float bias_linear[P] = { -0.1913,0.0259,0.1537,-0.0353,-0.1896,-0.1876,-0.1461,0.1443,0.1384,0.0078,-0.1283,0.0959,0.1429,0.0286,-0.1782,-0.1849,0.1776,0.0487,0.0587,-0.1135,0.0037,-0.0592,0.1909,-0.0393};
// Parameters to store and print the output of the linear layer + Queries [3x24]
	float concatenated_output_matrix_linear_pytorch[M][P] = {-0.3611,-0.0721,-0.0709,0.7592,0.0180,-0.4056,0.1496,0.3921,0.7612,0.2083,-0.2571,0.2865,0.2247,-0.0392,-0.2131,0.3503,0.1717,-0.3389,-0.2675,-0.0190,0.3019,-0.4531,0.1547,-0.0282,
															 -0.3009,0.1096,0.3107,0.6591,-0.0427,-0.4193,-0.2904,0.4048,0.3079,0.5793,0.0722,-0.0944,0.2095,-0.2320,0.0331,0.0802,0.3553,-0.1301,0.0123,-0.1711,0.1675,-0.2931,0.0890,-0.1029,
															 -0.3219,0.1359,0.3133,0.6907,-0.0122,-0.4279,-0.3354,0.4309,0.3124,0.5943,0.0863,-0.1087,0.2127,-0.2742,0.0871,0.0027,0.3741,-0.1335,0.0513,-0.2123,0.1701,-0.2302,0.0535,-0.1385};
	float sum_out_attention_queries[M][P]= {};

// Parameters of Norm1 & Norm2
	float norm_layer1[M][P] = {1.5770,-0.3067,0.2801,0.4576,-0.7575,-0.6178,0.5873,-0.0809,0.9214,1.5405,-1.6576,-0.4958,1.6888,0.0192,-0.5925,0.3001,0.1972,1.2610,-0.0136,-0.5782,-2.8310,-0.3657,-0.4415,-0.0915,
							   0.7380,1.2101,1.6532,0.4382,0.6505,-1.7373,-0.7284,1.3494,-0.9764,-0.3375,1.1698,-0.7026,0.8896,0.6525,0.6231,-1.5998,0.6163,0.1053,-1.7340,0.0617,-1.5595,-0.2864,-0.5126,0.0167,
							   1.1911,0.7104,0.8812,0.5570,-0.9841,-1.3347,0.1864,-0.2888,-0.6123,-0.3160,0.3298,-1.0574,-0.0671,0.4504,-0.0538,0.7418,-0.5264,-0.5107,1.2331,-0.7102,2.8525,-1.8879,-1.1298,0.3454};
	float norm_layer2[M][P] = {1.5530,-0.4316,0.1684,0.5051,-0.2302,-0.2065,0.2461,0.0979,0.3202,1.1178,-2.3478,-0.2078,1.9257,0.0424,-0.3121,0.2011,-0.1918,1.2263,0.2527,-0.0405,-2.8939,-0.2464,-0.3246,-0.2233,
							   0.8934,1.1199,1.3667,0.4157,0.7139,-1.1311,-0.9865,1.4413,-0.6493,-0.6593,0.6778,-1.0564,1.3047,1.0288,0.7101,-1.1534,0.3141,0.0987,-1.8691,0.3673,-1.8699,-0.2186,-0.8712,0.0123,
							   1.1860,0.3947,0.9775,0.4090,-1.2776,-0.9254,0.0306,-0.0326,-0.3472,-0.3634,-0.0149,-1.2392,0.1009,0.9675,-0.1318,0.6799,-0.6749,-0.5665,1.5170,-0.6234,2.6742,-1.7458,-1.3486,0.3541};
// Parameters of Feed Forward Layer
	float matrix_weights_linear1_fw[K][Q] = {-0.0031,-0.1844,-0.0993,0.1791,-0.0670,0.0943,0.0297,-0.1666,-0.1885,0.2017,-0.0975,0.0095,-0.1783,-0.1297,0.1253,-0.1998,0.1822,0.0193,0.0828,0.1364,0.1769,0.1342,-0.1459,-0.0605,-0.0737,-0.0865,0.1283,0.0986,0.1247,0.1389,0.0646,0.0661,0.0593,0.1745,-0.0435,0.1164,0.1193,0.1338,-0.0481,-0.0011,0.1994,-0.0958,-0.0345,0.0579,0.0378,0.1779,-0.0692,0.1336,0.1638,-0.0753,0.0762,-0.0502,0.1745,-0.1433,0.0126,-0.1844,0.1149,-0.1327,-0.1573,-0.0798,0.1674,0.1876,-0.0596,-0.0072,-0.0904,0.0476,-0.0897,0.0857,-0.1453,-0.0206,-0.1303,-0.1595,-0.0277,0.0580,0.1043,0.0437,-0.2029,0.0371,0.1479,0.0748,0.0534,0.1467,0.1965,-0.0538,0.0942,0.1430,0.0453,-0.0757,0.1070,-0.1877,-0.1933,-0.1150,-0.1607,-0.0812,-0.0259,-0.0434,0.1097,-0.1220,-0.1040,0.0931,-0.0419,-0.0438,0.0806,0.1454,0.0851,-0.1545,0.1025,-0.0449,0.0989,0.1667,-0.1704,-0.0900,0.0390,-0.0697,-0.1663,0.1733,0.0998,-0.1242,0.1832,-0.1409,0.0604,0.1405,0.0866,0.0673,0.0107,-0.1331,0.0067,-0.1034,-0.1067,0.0469,0.1394,0.1087,0.1085,0.1922,0.1154,0.1922,0.0181,-0.1483,0.1610,-0.0316,0.0277,-0.1443,0.0421,0.1707,0.1452,-0.0758,0.0411,0.1955,-0.1501,-0.1560,-0.0835,-0.1419,0.0996,0.1237,0.1254,-0.0383,0.1075,0.1500,-0.1217,-0.0291,-0.1626,0.1378,-0.1863,0.0268,-0.0860,-0.0596,-0.1967,-0.0223,0.0527,0.1794,0.1401,0.1442,0.0404,-0.1369,0.0950,0.0718,-0.0675,-0.1477,0.1553,-0.1543,0.0045,-0.1566,0.0196,0.1906,-0.2033,0.1765,0.0326,0.0704,0.0912,0.1264,-0.0611,0.0171,0.0672,-0.0243,0.1018,-0.1301,-0.1607,-0.1117,-0.0827,-0.0388,-0.1272,0.1908,-0.1036,0.1218,-0.1373,0.0015,0.0181,0.1774,-0.0884,0.1768,-0.1287,0.1268,-0.0146,0.0063,-0.1205,-0.0500,0.0662,0.1898,0.1177,0.1883,-0.0964,0.0446,-0.0047,0.1248,0.0194,-0.0000,0.1202,-0.0953,-0.1900,0.0617,0.1220,-0.1870,0.1966,-0.1412,-0.1997,-0.2040,-0.0980,-0.1389,-0.0987,0.0973,0.0683,0.1374,-0.0432,-0.0042,0.0402,0.1796,0.0504,0.1952,-0.1520,-0.1399,-0.1768,0.0641,0.1127,-0.0252,0.1764,0.1144,0.0512,0.1251,-0.0518,0.1961,0.0319,-0.1306,0.1829,0.0348,-0.1215,0.1388,-0.0034,0.0010,-0.1080,-0.1834,-0.0701,0.0099,-0.0799,0.1134,0.1935,0.0751,0.2001,-0.1764,-0.1504,0.0322,0.0845,0.1947,-0.0460,0.1245,0.1788,-0.1360,-0.0207,0.0271,-0.2015,0.1841,-0.0423,0.1078,-0.0257,0.1941,0.0607,-0.0216,0.0678,0.1136,0.1339,-0.0754,0.0836,-0.0414,0.0046,0.0486,-0.1154,-0.1595,0.0377,-0.1014,-0.1837,0.1190,0.0805,-0.1027,-0.0761,0.1764,0.1210,-0.0487,0.0030,0.0119,0.1460,0.0845,-0.0802,0.0957,-0.1561,0.1830,-0.1705,-0.0112,0.1361,0.1655,-0.0731,-0.1458,0.0304,-0.0723,-0.0029,-0.1369,0.1868,-0.0504,0.0615,-0.1520,-0.1487,-0.1576,-0.1762,-0.1700,0.1591,-0.0440,-0.0682,0.0224,0.0245,0.0006,-0.0731,0.1043,-0.0056,-0.0898,-0.1245,0.0782,-0.0651,0.1729,-0.1407,-0.1466,-0.0146,0.1299,0.1179,0.1256,0.0341,0.1811,0.0099,0.0491,0.0701,-0.0164,-0.0622,0.1023,-0.0292,0.1104,-0.1042,-0.1197,0.1504,-0.0940,-0.1781,0.1923,-0.0577,-0.1897,0.1285,-0.0258,-0.1860,0.1603,0.1049,0.1716,0.1246,0.0617,0.1243,0.0112,-0.1820,0.0610,0.0385,0.1006,0.0794,0.1916,-0.0171,0.1045,-0.0334,-0.1106,0.0744,-0.1382,0.0173,0.1925,-0.2032,-0.0791,-0.1027,-0.0728,0.1941,0.0216,0.0110,-0.0970,-0.2018,-0.1609,0.0181,-0.0419,-0.0183,0.1169,-0.0044,-0.0393,-0.1101,-0.0890,0.1697,0.1693,-0.0218,0.1254,-0.2014,0.1084,0.1874,0.0302,-0.0393,0.1746,-0.1517,-0.1607,0.0888,-0.0507,-0.2036,0.0298,-0.1256,0.1013,-0.0307,0.0817,0.0858,0.0965,-0.1563,0.1165,0.0333,0.0267,0.1429,0.0659,-0.0553,-0.0063,-0.0203,-0.1850,0.1032,-0.0698,0.1703,-0.1669,0.0824,0.0229,-0.0684,-0.1248,0.1318,0.1722,0.0327,-0.1601,-0.1479,0.0076,-0.1751,0.1847,0.0671,-0.1427,0.0998,-0.1046,-0.1608,-0.1743,0.0026,0.1334,0.1514,-0.0797,-0.1401,-0.0022,-0.0858,0.1674,0.1884,0.1267,-0.0817,-0.0010,0.1045,0.0339,-0.0775,-0.0800,-0.1893,-0.0836,0.1406,-0.1541,0.0361,0.1496,-0.1319,-0.0737,0.1813,0.1551,0.1108,0.1286,-0.1567,0.1822,-0.0192,0.0744,-0.1035,0.0457,0.0990,0.1749,-0.0270,0.0364,-0.1871,0.1387,-0.1360,0.0973,0.0284,-0.1666,0.1788,0.1752,-0.1843,-0.1391,0.0758,-0.0622,0.0971,0.1239,-0.0768,0.0847,-0.0015,-0.0060,0.1971,-0.0443,0.1325,-0.1951,0.1788,-0.0484,-0.1054,-0.0513,-0.1619,-0.1228,0.0866,-0.0334,-0.1732,-0.1969,0.0805,0.0783,0.1589,-0.1389,0.1947,-0.0727,0.0153,-0.1545,-0.0452,0.1517,0.0189,0.0691,0.1688,0.0219,-0.1835,0.0297,0.1633,-0.1102,0.1392,0.1895,0.1167,-0.1114,-0.1362,0.0669,0.1124,-0.0535,0.1387,0.0811,-0.1517,0.0898,-0.1196,-0.1368,-0.0299,0.0711,0.0076,0.1672,0.1134,-0.0533,0.1618,-0.0661,0.1962,0.1181,-0.1740,-0.0729,0.1696,-0.0004,0.1275,0.0626,0.0066,-0.0563,0.1307,0.2034,0.1655,-0.2029,-0.0538,0.0100,0.0816,-0.0152,0.0736,0.1442,-0.0573,-0.0744,-0.1469,0.1863,-0.0879,-0.0294,0.1289,-0.0732,0.0179,0.1616,-0.1626,0.0591,0.0401,0.0722,-0.0726,-0.1608,-0.1036,-0.0607,0.1134,-0.0215,-0.1323,-0.0026,-0.1159,0.0839,-0.1381,0.0891,0.1052,-0.0471,0.0955,0.1549,0.0650,-0.1182,0.1829,0.0782,0.0324,0.1256,0.0794,-0.1625,-0.1891,-0.0681,0.0859,-0.1182,0.1314,-0.0880,-0.0517,0.0944,-0.0082,0.1008,0.1133,-0.0420,0.0209,-0.1754,-0.0014,0.0217,-0.1876,-0.1699,0.1524,0.0254,0.0641,-0.1013,-0.0427,0.0913,0.0044,0.1108,0.0225,-0.1154,0.0002,-0.0650,-0.0667,0.1332,0.1682,0.0708,0.1956,-0.0917,0.0891,-0.1761,0.0241,-0.0070,-0.0847,-0.1683,-0.0265,0.0837,0.0832,0.1533,0.0047,-0.1700,0.0422,0.1459,0.0426,0.1648,-0.2025,-0.0542,-0.1094,-0.1566,0.0891,-0.0332,0.1217,0.1830,-0.1580,-0.0895,0.1599,0.0286,0.0673,-0.1439,-0.1133,0.0908,-0.1869,-0.0458,-0.0671,-0.1463,-0.1960,-0.1054,0.0182,-0.0130,-0.0121,0.1549,-0.0956,-0.0323,0.1221,0.1631,-0.1611,0.1437,-0.1868,-0.1171,-0.1508,-0.1001,-0.1734,-0.0700,-0.1253,-0.0458,0.1512,0.0208,0.1715,-0.0993,-0.0093,-0.0420,-0.0847,-0.1138,0.0187,-0.1740,-0.0447,0.0621,-0.0944,0.0609,-0.1347,-0.1355,0.1719,0.1375,-0.1814,0.1107,0.0014,-0.1856,0.0672,-0.0710,-0.0913,-0.1446,-0.1857,0.1489,0.1275,-0.1086,-0.1157,0.1067,0.0576,0.0169,0.0846,-0.1312,-0.1160,0.0435,-0.0218,0.0089,0.0864,0.0656,-0.1300,0.1276,-0.1236,-0.2020,-0.1471,0.0720,0.0421,-0.1581,-0.1173,0.0765,-0.1237,-0.1625,0.1687,0.0528,0.1044,0.1516,0.1168,0.0472,0.1234,-0.1722,-0.1989,0.0305,0.1100,0.1201,-0.0047,0.1919,-0.1160,0.0745,0.1763,-0.0070,-0.0382,0.1111,0.0936,-0.1692,-0.0770,0.0236,0.1174,-0.0229,0.1881,0.0181,-0.0181,0.1535,-0.0851,0.1479,0.1781,0.1198,0.1471,0.1380,-0.0098,0.0439,0.1635,0.0568,-0.1313,0.0133,0.0795,-0.0990,-0.0101,0.1271,0.1601,-0.0606,0.0009,0.0878,-0.0051,-0.0034,0.1123,-0.0848,-0.1264,-0.1140,0.0873,0.0013,-0.1927,0.0600,-0.1406,0.1166,0.0469,0.1906,0.1625,-0.0012,-0.0506,-0.1312,0.1467,-0.1095,-0.0902,0.1836,-0.0104,-0.0742,0.1575,0.0741,-0.0301,0.0357,-0.1249,0.0595,-0.1538,0.0163,0.1956,-0.1591,-0.1947,0.1616,-0.1418,-0.1829,-0.0722,-0.0323,0.1802,0.0920,-0.0852,0.0251,-0.1791,-0.0323,-0.1092,-0.0383,-0.1669,0.0644,-0.0675,-0.0641,-0.1848,0.0125,0.1601,0.1369,-0.1500,-0.1085,-0.1047,-0.1782,0.1215,-0.0399,0.0657,0.1450,-0.0540,0.1553,-0.0944,0.0278,0.0936,0.1781,-0.1240,0.1917,-0.0998,0.1520,0.0873,0.1505,0.0118,-0.1757,-0.0051,-0.0660,0.1956,0.1535,0.1940,-0.1062,-0.2008,0.0033,0.0989,-0.1264,0.1461,-0.0506,-0.1008,0.1083,-0.1822,-0.1237,-0.1330,-0.0517,-0.1624,0.1935,0.0748,-0.1844,0.0252,0.1818,-0.1923,-0.1642,0.1460,0.0248,-0.0470,-0.1355,-0.0145,-0.1984,-0.0819,0.0392,0.1775,-0.1290,0.0226,-0.1355,-0.0314,0.0210,-0.1575,0.0270,-0.0378,-0.0655,-0.0000,-0.1705,0.1693,-0.0273,-0.1454,0.0633,0.1726,0.1363,-0.1332,-0.1147,0.0042,0.1511,0.1959,-0.1292,-0.1944,-0.1859,-0.0594,-0.1019,-0.1570,0.0786,-0.1946,0.1315,-0.1257,0.1885,-0.0691,-0.1130,-0.0521,0.0364,0.1187,0.0793,0.0699,-0.0026,-0.1064,-0.1806,0.0810,0.0616,-0.1685,0.0091,0.1614,-0.0122,0.0748,-0.0840,-0.0350,-0.1150,-0.0121,-0.0630,-0.0110,-0.0628,0.0394,0.0422,-0.1239,0.1685,0.0203,-0.1930,0.0341,0.0844,0.1878,0.0549,0.1434,0.0526,0.1337,0.0329,0.0328,-0.1978,0.0409,0.1008,0.1429,0.1741,-0.1902,-0.1798,-0.1826,0.0704,-0.0472,0.0132,-0.1525,-0.0932,-0.0784,-0.0860,-0.0231,-0.1255,0.1747,0.0792,0.0310,-0.1259,-0.0296,0.1891,-0.0541,0.1804,-0.0479,0.0647,-0.1420,0.1805,-0.0036,-0.1244,0.1035,0.0553,0.0186,0.1559,0.1892,-0.0170,0.1245,0.1093,0.1601,0.0014,-0.1626,0.1426,-0.1648,0.0711,-0.0099,-0.0589,0.1549,-0.0285,-0.1178,0.0003,0.0458,0.1196,-0.0614,-0.0162,-0.0975,0.0224,0.1657,0.1523,-0.1625,0.0747,-0.1892,-0.0118,-0.0107,0.0137,-0.0393,0.0985,0.0564,0.0770,-0.1683,0.1106,-0.0572,-0.0425,0.1484,0.1727,0.1707,-0.1556,-0.1319,0.0071,0.1474,-0.0835,0.1177,0.0617,0.0266,0.0103,0.1163,0.1218,0.0986,-0.0601,0.1212,-0.0561,-0.1958,0.0600,0.1394,-0.1551,0.1880,-0.0053,0.1828,0.1740,-0.0736,-0.1422,-0.1608,-0.0267,-0.1399,0.0082,0.0885,-0.1416,0.0094,0.0888,0.0119,0.0009,-0.1777,-0.0158,0.0233,0.0402,0.1094,-0.1090,-0.1111,0.1604,-0.0253,0.1195,0.0418,0.1738,0.1142,0.0323,-0.0853,-0.0005,0.1011,0.1967,0.0277,-0.0028,0.1673,-0.0555,-0.0339,-0.1551,0.0587,-0.0474,-0.0796,0.0216,0.1747,-0.1153,0.1200,-0.1261,-0.0745,0.0963,0.0181,0.0311,0.0776,0.1132,-0.0923,-0.1018,0.1654,0.1190,-0.0967,0.1446,-0.0810,-0.0984,-0.1409,-0.0734,0.1903,-0.1090,-0.0952,-0.0098,0.1509,0.2040,-0.1772,-0.1917,-0.0655,-0.1469,0.0279,0.1775,0.0262,0.1266,-0.0869,0.0753,-0.0380,-0.1422,-0.1756,0.0166,0.0884,-0.1959,0.0838,0.1606,0.1891,-0.1970,-0.1090,-0.0741,0.1314,0.0229,-0.0923,0.1781,-0.1880,0.1949,-0.1071,0.0137,0.0250,-0.0240,0.0618,-0.0312,-0.1506,-0.1177,-0.0512,-0.1673,-0.1481,0.0185,0.0358,-0.1163,-0.0191,0.0318,-0.1453,0.0401,-0.0370,0.0738,0.1244,0.1212,0.0232,-0.0317,-0.1486,-0.0840,0.1603,0.0404,-0.0648,0.0339,0.0116,-0.1222,0.0881,0.0251,0.2009,0.1857,0.0555,-0.0372,-0.1443,-0.1004,-0.0202,0.0925,0.0906,0.1240,0.2030,0.1307,0.0023,0.0320,-0.0801,-0.0478,-0.0932,0.0891,-0.0318,0.0884,0.1427,0.1295,-0.1601,-0.1068,-0.0324,0.1517,-0.0879,-0.1426,0.0361,-0.0448,-0.1772,-0.0222,-0.0803,0.1586,-0.1280,0.0046,0.1581,0.1608,-0.1337,-0.0659,-0.0485,-0.0616,0.0567,-0.1759,0.0475,0.1817,0.1814,-0.0093,-0.1994,0.0188,-0.0793,-0.0097,0.1429,0.0067,-0.1079,0.0131,-0.0396,-0.0884,-0.0383,0.0882,0.0151,-0.1945,0.0313,-0.1263,-0.0955,-0.0477,-0.1219,0.0817,-0.0481,-0.1305,0.0787,0.1869,0.0508,-0.0156,-0.0289,-0.2010,0.0329,0.1688,-0.0813,-0.0274,0.1892,-0.1938,0.0372,-0.1144,-0.1167,0.1227,0.1705,0.1270,-0.0078,-0.0613,0.1485,0.1013,-0.0091,0.1438,0.1610,0.1082,0.0924,-0.1188,0.1999,-0.0017,0.1702,0.1583,-0.0965,-0.0455,-0.0119,0.0703,0.1484,0.1085,0.0100,0.0265,-0.1770,-0.1912,0.0842,0.0593,-0.1468,-0.1200,-0.0917,-0.1974,0.0573,0.0802,0.0684,0.0699,-0.1890,0.0064,-0.1593,-0.0775,0.0735,-0.1503,0.1340,-0.0144,0.0252,0.0077,0.0977,0.1221,0.0662,-0.2027,-0.0717,0.0028,-0.1575,-0.0022,0.0960,0.0104,0.0589,-0.1248,-0.0614,0.0195,-0.1345,-0.0826,0.1594,-0.1450,-0.0874,-0.1543,0.0819,-0.0658,-0.0788,0.0102,-0.1285,-0.0434,0.0858,0.1232,-0.0749,-0.0851,0.0126,-0.0504,-0.0899,0.1782,-0.1333,0.2020,-0.0462,-0.1169,0.0086,
 	    										0.1194,0.0817,-0.1107,-0.0892,-0.0489,-0.1424,0.0961,-0.1790,-0.0202,-0.1477,0.0518,-0.0974,-0.0229,0.1929,0.0278,0.0537,0.0426,-0.1519,-0.1123,0.1751,-0.1283,0.0462,0.0882,-0.0632,0.1483,-0.0203,-0.0650,-0.1811,0.1824,0.1441,-0.1382,-0.0398,0.0581,0.0751,0.0439,-0.0474,0.1116,0.0352,0.1758,-0.1201,-0.1521,0.1481,-0.1468,0.1104,-0.1363,0.1389,-0.0333,-0.1114,-0.1207,0.1986,-0.1243,0.1685,-0.1645,-0.0114,0.1004,0.0035,0.0336,0.0479,0.0872,-0.1339,0.0399,0.0915,-0.1322,0.0464,0.1362,-0.1034,0.0645,-0.0313,-0.0247,0.0673,-0.1521,0.1810,0.1490,0.1903,0.0136,0.0366,0.0173,0.1189,0.0486,-0.1431,-0.1720,0.0881,0.0779,-0.0410,0.0390,-0.0173,-0.0347,0.1264,-0.1222,0.1960,-0.1727,0.0211,0.0747,-0.0405,0.1923,-0.0407,0.0198,-0.1489,0.1829,0.0021,-0.0999,0.1216,0.0969,-0.0203,0.1785,0.0645,0.0597,-0.0275,0.0979,-0.1161,0.1617,-0.1115,0.1310,0.1136,0.0109,-0.1516,-0.1289,0.2001,0.0889,0.0470,0.0552,-0.1448,-0.0272,-0.1243,0.1749,-0.0128,0.1625,0.1905,0.1955,0.0978,-0.1455,-0.0807,-0.0312,0.0611,0.0992,-0.1716,-0.1797,-0.1257,-0.0866,-0.1066,-0.1935,0.0728,0.0898,-0.0124,0.0420,0.1553,-0.0020,0.0011,0.1577,0.0161,-0.0760,0.0725,0.0301,0.1542,0.0126,-0.1284,-0.1747,0.0950,0.1692,-0.2035,0.0300,0.1485,0.0391,0.0071,0.0051,-0.2035,-0.0695,-0.1506,0.1815,0.1022,-0.1032,-0.0943,-0.1956,-0.1396,0.1832,0.0535,0.1263,0.1348,-0.0644,-0.1498,-0.0594,-0.1845,-0.1070,-0.1532,0.1979,-0.0635,0.1269,0.0075,-0.2025,0.1560,0.0182,0.0648,0.0300,-0.0922,-0.1967,-0.1499,0.0545,-0.1363,-0.0349,-0.0753,-0.1589,0.0470,-0.1820,-0.1899,-0.0524,0.1786,0.0140,-0.1559,0.1489,-0.0049,-0.1674,-0.1467,0.0159,0.0308,-0.0856,0.0021,0.0253,0.1645,-0.1364,0.1725,-0.0437,-0.1583,0.1268,0.0907,0.1145,0.0055,0.0729,-0.1067,0.1679,-0.1616,-0.0649,0.0920,0.0750,-0.2011,0.0259,-0.1497,-0.0795,0.0581,0.1787,0.1122,0.0411,0.1838,0.0738,0.0054,-0.1812,-0.1115,-0.1647,-0.2031,0.0658,0.0519,-0.1534,0.0975,0.1135,0.0900,0.1540,0.0784,-0.0222,-0.0265,-0.1508,-0.1384,0.1727,0.0175,-0.1882,0.0731,0.1576,-0.0228,0.0199,0.1969,-0.2020,-0.1200,-0.1985,-0.0549,-0.1715,-0.0016,-0.0207,0.1337,0.1359,0.1820,0.0157,-0.0643,0.1009,-0.1460,0.1779,0.1536,0.0645,0.0595,0.1330,-0.1531,0.0975,0.0865,0.0905,-0.1126,0.0600,0.0906,0.2011,-0.0050,0.1555,0.0148,0.1079,0.1075,-0.1947,0.0827,-0.1065,0.1577,0.0963,0.0486,-0.0453,-0.1664,0.0367,-0.1002,-0.0723,-0.1571,-0.0991,-0.0008,-0.0855,0.0676,-0.0679,0.1302,0.0805,-0.0977,-0.1840,0.2034,0.1866,-0.1268,0.0296,0.0991,0.1387,-0.0782,-0.0135,0.1794,-0.0840,0.1731,0.1030,-0.1548,-0.1699,0.1921,0.1502,0.1249,-0.1814,0.1606,0.0876,0.0458,-0.0063,-0.1909,0.1296,0.1363,-0.1792,-0.0446,-0.1168,-0.1330,0.1955,-0.0411,0.0318,0.1344,-0.1856,0.1427,0.1785,-0.0756,-0.0246,-0.0253,-0.1513,0.1125,0.1750,0.1561,0.1133,-0.0548,0.0872,0.0193,0.1886,0.0233,0.1805,0.0642,0.0520,-0.0957,0.0750,-0.0127,0.0757,0.0791,-0.1350,0.0765,-0.1341,0.1939,0.1378,-0.0518,-0.1906,0.1916,-0.1706,-0.0174,0.0077,0.1163,0.0606,0.0309,-0.0213,-0.1052,-0.1077,0.0934,-0.1292,0.1661,-0.1742,-0.0458,0.1389,0.0305,0.1847,-0.0886,-0.0464,0.0154,0.1558,0.1501,0.0317,-0.1280,-0.1188,0.1995,0.0487,-0.0716,-0.0923,-0.1252,0.1867,0.0674,-0.1916,-0.0723,0.1058,-0.1468,0.0193,0.1933,-0.0055,0.0229,-0.2040,-0.0146,0.0141,0.1502,-0.1310,-0.1440,0.0103,0.0832,-0.1889,0.1641,-0.1435,-0.1039,0.2003,-0.1705,-0.0057,0.1946,-0.1747,0.1470,-0.0852,0.1895,-0.1175,0.0669,-0.1445,0.1455,0.0823,-0.2001,-0.0183,-0.0575,-0.0758,0.1102,0.1741,0.1924,-0.1871,-0.1162,-0.1600,0.0594,0.1823,-0.0545,-0.0871,-0.1694,-0.2013,0.1353,0.0381,-0.0064,-0.1845,-0.0645,-0.0330,-0.1857,-0.1298,-0.1142,-0.1363,0.2006,0.1630,0.0463,-0.1897,0.0101,0.1492,-0.1515,0.1090,0.0970,0.1126,0.0646,0.1772,-0.0374,-0.1612,0.1652,0.0379,-0.1130,-0.0783,-0.1142,0.1852,0.1149,-0.1594,-0.1877,0.0716,0.0194,-0.1556,-0.0188,-0.1317,0.0660,-0.2041,-0.1343,-0.0179,0.0429,-0.0467,0.0238,-0.1633,-0.1922,-0.0015,-0.0725,0.0861,0.0930,0.0730,0.1854,-0.1423,0.2015,-0.1348,-0.1883,0.0051,0.1034,0.1079,0.1979,0.0591,0.1013,-0.0680,-0.1368,-0.0515,0.1302,-0.1183,-0.0263,-0.1717,0.0632,0.1186,-0.1333,0.1208,0.0080,0.1521,0.1639,0.0788,0.1747,-0.0385,0.2032,0.0636,0.1128,0.1138,-0.1993,0.1766,-0.1819,-0.1818,0.0323,0.1531,-0.2002,0.0284,0.1054,-0.0781,-0.0498,-0.1907,0.0802,-0.1997,-0.0827,-0.0715,-0.1645,0.0981,0.0437,0.0492,0.0997,-0.0567,0.0583,-0.0362,-0.1521,0.1330,-0.0657,-0.0410,-0.0764,-0.1535,-0.0163,0.0682,0.1516,0.0506,-0.1695,-0.1151,-0.0842,0.1664,-0.0492,-0.0338,0.0330,-0.0770,0.1990,-0.1858,-0.0034,0.0731,-0.0889,-0.0851,0.0452,-0.0100,0.0124,0.0258,0.1420,0.0467,0.0301,0.0192,-0.1058,-0.0226,0.0292,0.1668,0.1715,-0.0356,-0.2000,0.0484,-0.1523,0.1953,-0.0547,-0.0107,0.0573,0.0896,-0.0045,0.1651,-0.0775,0.0692,-0.0448,-0.1262,-0.0406,-0.1732,-0.1319,-0.1333,0.0210,0.0003,-0.0218,0.1662,0.0068,0.1568,-0.0984,0.1621,-0.0534,0.1153,-0.1102,0.0294,-0.1712,-0.0215,0.1521,0.0876,0.0308,-0.0899,-0.1740,-0.1741,-0.1838,-0.1089,-0.0641,0.1003,0.1061,-0.1967,-0.0453,0.0483,-0.1083,-0.0242,0.0531,0.0482,-0.0466,0.0500,-0.1194,-0.1242,0.1949,0.1381,0.2037,-0.0443,-0.0462,-0.0406,0.0073,-0.0303,0.0977,0.1892,0.1531,0.1941,-0.0015,0.1275,-0.1213,-0.1632,0.0716,-0.1348,-0.0281,0.0573,0.1528,-0.0393,0.0107,-0.0167,0.0060,0.0690,-0.1030,0.1535,0.0379,0.1801,-0.0827,-0.0722,-0.1695,-0.0122,0.0057,0.1494,-0.0482,0.1586,0.0324,-0.1704,0.1197,-0.1879,-0.0382,0.1153,-0.1523,-0.1653,0.1385,0.1118,0.0988,-0.0937,0.0389,-0.0024,-0.0881,-0.1410,-0.0445,0.0892,-0.1629,-0.1018,0.1489,-0.1373,-0.1029,-0.1277,0.1821,-0.0225,0.1388,-0.1091,0.0341,-0.0644,0.1069,-0.1512,0.0212,0.0306,-0.0557,-0.1060,0.0472,0.1959,0.1984,0.1426,-0.1519,0.1041,-0.1761,-0.1107,-0.0832,0.1986,0.1407,-0.0459,0.0826,0.1847,-0.0676,-0.0245,-0.0982,0.1980,0.0997,0.1331,0.1572,-0.0346,0.1284,0.1280,-0.0197,0.1587,-0.0121,-0.0907,-0.2012,-0.1475,0.0552,-0.1352,0.1720,-0.0663,0.0951,0.1371,0.0250,0.0521,-0.1746,0.2023,0.0388,0.0929,-0.1641,0.1946,-0.0463,0.0261,-0.1788,0.1495,-0.1812,-0.0367,0.0881,0.0178,0.0350,-0.1454,0.1708,0.0111,-0.1696,-0.1652,-0.1343,0.1048,0.1753,0.1498,0.0822,-0.0253,-0.1621,-0.0716,0.0966,-0.1061,-0.0842,0.0522,-0.1850,-0.0730,-0.0132,-0.1245,0.0079,-0.0226,-0.1213,0.1401,-0.1705,0.0999,0.2025,0.1746,0.0427,0.0116,0.1074,-0.0137,-0.1127,-0.1820,-0.2034,0.1033,-0.0631,-0.1499,0.0944,-0.1700,0.1005,-0.0424,-0.1636,-0.0401,-0.1622,-0.0560,-0.0956,-0.1642,0.2019,0.0048,-0.1476,0.1519,0.0406,0.1073,-0.1037,0.1351,-0.1651,-0.0910,-0.1140,-0.0894,-0.1477,0.0455,-0.0293,0.0741,-0.0450,-0.0704,0.0238,-0.1291,-0.1885,0.1398,-0.1342,0.1322,-0.1692,-0.1112,0.2016,0.1879,-0.0298,-0.0395,-0.1827,0.1735,0.0788,-0.0407,-0.1658,0.1397,0.1830,0.1507};
	float bias_linear1_fw[K] = {-0.0466,-0.0534,-0.0358,0.0826,0.1452,-0.1598,-0.0003,0.1344,0.0459,-0.1846,0.1476,-0.1531,0.1445,-0.1823,-0.1601,0.1363,0.1397,0.0991,0.1667,0.1672,0.1379,-0.1692,-0.0190,0.1515,0.0860,0.0783,0.0622,-0.1145,-0.1552,-0.0077,0.0984,0.0326,-0.1345,0.0796,0.0592,-0.1867,0.1844,0.0705,0.0174,0.0059,0.1636,0.0660,-0.0170,-0.1029,0.1168,0.1238,-0.0562,-0.1463,0.1604,0.1609,-0.0414,-0.0475,-0.0403,-0.0697,-0.1770,-0.1788,0.0249,0.1648,-0.1554,-0.0266,0.1115,-0.0393,-0.1764,-0.0003,0.0188,-0.0272,-0.1096,-0.0112,0.0022,-0.0577,-0.1760,-0.1006,-0.1072,0.1955,-0.1739,-0.0430,0.1299,0.1691,-0.1572,0.0129,0.1964,0.1639,0.0728,0.1385,0.0917,0.1701,-0.1095,0.1721,0.1959,-0.0100,0.0931,-0.1373,0.0350,-0.0482,-0.1232,0.1001};
	float matrix_weights_linear1_fw_t[Q][K] = {};
	float matrix_weights_linear2_fw[Q][K] = {0.1009,-0.0058,-0.0855,0.0229,-0.0519,-0.0137,-0.0977,0.0731,-0.0633,0.0258,-0.0010,-0.0603,-0.0607,0.0507,-0.0033,-0.0845,0.0291,0.0699,-0.0713,-0.0350,0.0490,-0.0934,0.0367,-0.0403,0.0005,0.0499,0.0190,-0.0572,0.0251,-0.1000,0.0796,0.0050,-0.0149,-0.0309,0.0932,0.0029,0.0874,-0.0201,-0.0044,0.0476,-0.0389,-0.0011,0.0898,0.0800,-0.0355,-0.0541,0.0364,0.0547,-0.0371,0.0044,0.0577,-0.0507,-0.0248,0.0534,0.0692,0.0226,0.0241,0.0204,-0.0793,0.0525,0.0039,0.0479,0.0389,-0.0972,0.0673,-0.0903,0.0318,0.0361,0.0474,-0.0816,-0.0712,-0.0534,0.0211,0.0757,-0.0100,-0.0789,0.0496,0.0021,-0.0818,0.0750,-0.0366,0.0555,0.1019,-0.0360,-0.0576,-0.0254,0.0746,0.0148,-0.0852,0.0972,-0.0257,0.0688,0.0933,-0.0986,-0.0228,-0.0093,0.0544,0.0387,-0.0018,0.0360,-0.0232,-0.0625,-0.0187,0.0397,0.0629,-0.0732,0.0071,0.0381,-0.0158,-0.0312,0.0408,-0.0618,-0.0806,-0.0148,0.0586,0.0092,0.0549,-0.0127,-0.0901,0.0114,-0.0825,0.0053,-0.0782,0.0837,-0.0039,0.0376,0.0789,-0.0172,-0.0038,0.0364,-0.0860,-0.0146,0.0107,-0.0757,0.0518,0.0940,-0.0579,0.0815,-0.0542,0.0655,-0.0862,-0.0792,-0.0312,-0.0802,0.0185,0.0074,-0.0060,0.0629,0.0973,0.0287,-0.0400,0.0005,0.0328,-0.0312,-0.0972,-0.0533,0.0554,0.0065,0.0534,-0.0791,0.0977,-0.0341,0.0548,-0.0745,-0.0261,0.0628,0.0080,-0.0970,-0.0422,-0.0992,0.0444,-0.0335,0.0495,0.0665,0.0473,0.0270,0.0280,-0.0236,-0.0565,0.0514,-0.0460,-0.0241,0.0128,0.0183,-0.0067,0.0383,-0.0877,0.0023,-0.0604,0.1011,0.0981,0.0194,-0.0270,-0.0877,-0.0493,0.0494,-0.0202,0.0102,-0.0118,-0.0596,0.0819,-0.0987,0.0282,0.0592,0.0227,0.0565,0.0468,0.0890,0.0340,-0.0827,0.0452,-0.0980,-0.0965,-0.0436,0.0382,-0.0636,-0.0080,0.0651,-0.0267,0.0647,-0.0252,-0.0321,-0.0269,0.0102,0.0537,-0.1012,-0.0777,-0.0037,0.0798,0.0028,0.0517,0.0325,0.0064,-0.0375,-0.0866,0.0500,0.0756,0.0276,0.0697,-0.0796,0.1017,-0.0270,0.0809,-0.0176,-0.0480,0.0278,-0.0839,0.0454,-0.0855,-0.0628,0.0220,0.0899,-0.0223,0.0228,0.0641,-0.0832,0.1007,0.0694,-0.0537,-0.0383,-0.0841,0.0838,-0.0696,0.0628,-0.0275,0.0306,-0.0254,0.0856,-0.0577,0.0905,0.0966,0.0035,-0.0177,0.0278,-0.0913,-0.0719,-0.0598,-0.0991,-0.0476,0.0291,0.0590,0.0365,-0.0103,0.0758,0.0207,-0.0693,0.0702,-0.0788,-0.0232,-0.0830,0.0550,0.0274,-0.0107,-0.0631,0.0331,0.0602,0.0416,-0.0400,-0.0044,0.0371,-0.0251,-0.0839,-0.0281,-0.0072,-0.0804,-0.0852,-0.0429,0.0190,-0.0811,0.0551,0.0961,0.0167,-0.0689,-0.0203,0.0593,-0.0695,-0.0971,-0.0337,-0.0759,-0.0229,-0.0526,-0.0325,-0.0935,0.0216,0.0796,0.0032,0.0205,-0.0944,-0.0786,-0.0675,-0.0858,0.0748,-0.0454,-0.0676,0.0324,-0.0187,-0.0510,-0.0037,0.0872,0.0631,0.0244,-0.0970,0.0367,-0.0146,0.0142,0.0208,0.0164,-0.0808,-0.0322,-0.0810,-0.0493,0.0531,-0.0538,0.0033,0.0130,0.0511,0.0602,-0.0437,0.0498,0.0949,-0.0252,-0.0788,-0.0948,-0.0950,-0.0440,0.0871,-0.0750,-0.0529,-0.0481,-0.0822,0.0500,0.0309,0.0307,-0.0671,0.0228,-0.0923,-0.0550,0.0709,0.0556,-0.0184,0.0151,0.0765,0.0593,-0.0881,0.0424,-0.0316,-0.0729,0.0534,-0.0567,0.0526,0.0070,-0.0370,0.0508,-0.0988,-0.0364,-0.1011,0.0294,-0.0752,0.0579,-0.0707,0.0298,0.0260,0.0332,0.0782,0.0207,-0.0221,-0.0128,-0.0892,0.0315,-0.0735,0.0003,0.0240,-0.0109,-0.0882,0.0500,0.0079,-0.0741,0.0027,0.0356,0.0460,0.0743,-0.0982,-0.0225,-0.0562,0.0657,0.0966,-0.0531,0.1000,0.0357,-0.0855,0.0109,0.0951,0.0159,0.0165,-0.0094,0.0831,-0.0203,0.0490,-0.0257,0.0959,-0.0912,-0.0064,0.0406,-0.0928,-0.0118,0.0850,-0.0657,0.0997,0.0516,0.0505,0.0833,-0.0456,-0.0083,-0.0937,0.0363,0.0434,-0.0958,0.0513,-0.0414,-0.0383,-0.0229,-0.0345,-0.0905,0.0586,0.0642,0.0153,-0.0215,0.0281,0.0641,0.0736,0.0776,0.0989,0.0025,0.0402,0.0568,0.0814,-0.0311,0.0047,0.0046,-0.0370,0.0783,-0.0159,0.0241,0.0341,0.0253,-0.0918,0.0535,-0.0053,0.0335,0.0607,-0.0460,0.0834,-0.0894,0.0821,-0.0619,-0.0538,0.0828,0.0321,0.0854,0.0341,0.0362,0.0652,-0.0180,-0.0975,0.1012,0.0937,0.0516,-0.0893,0.0198,-0.0700,0.0791,0.0156,0.0873,0.0617,0.0277,-0.0997,0.0317,0.0853,0.0456,-0.0445,0.0885,0.0217,0.0699,-0.0030,0.0907,-0.0167,-0.0096,0.0815,-0.0711,-0.0084,0.0072,0.0311,-0.0266,-0.0487,0.0585,-0.0813,0.0472,-0.0957,0.0980,0.0996,-0.0425,0.0155,0.0631,-0.0988,0.1003,0.0612,0.0308,-0.0498,0.0625,-0.0052,0.0480,0.0427,-0.0956,0.0570,-0.0790,0.0787,0.0165,0.0715,0.0777,0.0710,0.0768,-0.0537,0.0218,-0.0066,-0.0527,-0.0495,0.0975,-0.0674,0.0208,-0.0929,-0.0058,-0.0446,0.0189,0.0934,0.0141,0.0511,-0.0881,-0.0507,-0.0123,0.0571,-0.0828,0.0697,0.0667,-0.0267,-0.1004,0.0804,-0.0649,0.0883,0.0493,0.0729,-0.0279,0.1010,0.0399,-0.0081,0.0051,-0.0979,-0.0638,0.0152,0.0597,0.0943,0.0228,0.0060,0.0241,0.0515,-0.0612,-0.0885,-0.0637,-0.0926,-0.0751,-0.0056,0.0622,0.0348,-0.0892,0.0397,-0.0947,-0.0390,-0.0632,-0.1004,0.0295,0.0546,0.0857,-0.0605,-0.0236,0.0164,-0.0275,-0.0296,-0.0288,0.0993,0.0671,0.0983,-0.0862,-0.0850,-0.0378,-0.0657,0.0308,-0.0566,-0.0061,-0.0783,0.0482,0.0399,-0.0488,0.0057,0.0246,-0.0288,-0.0857,0.0188,-0.0646,-0.0950,-0.0510,-0.0574,-0.0360,-0.0318,-0.0140,-0.0033,-0.0441,-0.0489,-0.0204,0.0295,0.0444,-0.0801,-0.0191,0.0831,-0.0770,-0.0766,-0.0631,-0.0740,0.0449,-0.0582,-0.0908,-0.0260,-0.0163,0.0257,0.0650,-0.0883,0.0611,-0.0518,0.0938,0.0185,-0.0805,0.0894,0.0106,-0.0873,0.0885,0.0912,-0.0056,-0.0132,-0.0106,-0.0934,-0.0971,0.0246,0.0905,0.0700,-0.0466,0.0947,0.0982,0.0053,0.0219,-0.0514,-0.0506,-0.0410,-0.0713,-0.0769,0.0527,0.0676,0.0287,-0.0337,0.0963,0.0099,0.0420,-0.0597,0.0830,-0.0512,0.0538,-0.0482,0.0339,-0.0494,0.0882,0.0699,-0.1007,0.0922,0.0630,-0.0322,-0.0430,0.0170,-0.0928,-0.0904,-0.0232,-0.0349,-0.0337,0.0083,0.0399,-0.0835,0.0655,0.0958,0.0872,0.0693,-0.0719,-0.1011,-0.0192,0.0272,-0.0203,-0.0476,0.0943,0.0663,0.0489,-0.0722,0.0639,0.0357,0.0556,-0.0594,-0.0215,0.0480,-0.0482,-0.0193,-0.0937,0.0690,0.0137,-0.0634,0.0710,-0.0666,0.0483,-0.0087,0.0047,-0.0622,-0.0546,-0.0061,-0.0159,0.0884,0.0429,0.0600,-0.0277,-0.0887,0.0875,0.0172,-0.0951,-0.0206,0.0867,0.0175,-0.0906,-0.0605,-0.0739,0.0951,-0.0895,0.0489,0.0718,-0.0023,0.0882,0.0552,0.0423,0.0330,-0.0493,0.0758,-0.0987,-0.0375,-0.1011,0.0617,-0.0403,0.0138,0.0630,-0.0544,-0.0772,0.0776,0.0338,0.0618,-0.0027,-0.0988,0.0821,-0.0298,-0.0748,0.0733,-0.0661,-0.0293,0.0373,0.0725,-0.0840,-0.0245,0.0890,0.0213,-0.0911,0.0625,-0.0425,-0.0476,-0.0425,0.0827,0.0168,0.0515,0.1006,0.0432,-0.0700,-0.0031,-0.0243,-0.0996,0.0957,-0.0303,0.0630,0.0334,0.0658,0.0292,0.0460,0.0318,0.0758,0.0983,-0.1008,0.0018,0.0954,-0.0010,-0.0511,0.0618,-0.0300,-0.0151,0.0053,0.0863,-0.1003,0.0008,0.0334,0.0037,-0.0793,-0.0133,-0.0642,0.0568,-0.0726,-0.1011,-0.0987,-0.0818,0.0942,-0.0371,0.0714,-0.0839,0.0879,-0.0715,-0.0830,0.0263,-0.0141,0.0527,-0.0920,0.0052,-0.0270,-0.0898,0.0195,0.0743,0.0708,0.0233,0.0119,0.0604,-0.0666,0.0178,-0.0582,-0.0864,0.0027,0.0124,0.0954,-0.0285,-0.0587,-0.0789,-0.0901,-0.0099,-0.0762,-0.0698,0.0651,-0.0987,0.0204,-0.0260,-0.0821,0.0756,-0.0664,-0.0912,0.0943,-0.0246,-0.0316,0.0233,-0.0896,0.0081,-0.0448,-0.1020,-0.0169,-0.0993,-0.0945,-0.0884,0.0266,0.0446,-0.0652,-0.0561,0.0442,0.0170,0.0503,0.0700,0.0300,-0.0839,0.0956,0.0251,-0.0236,-0.0873,-0.0891,0.0350,-0.0731,0.0373,-0.0038,0.0237,-0.0797,-0.0111,-0.0509,0.0448,0.0543,-0.0781,0.0286,-0.0284,0.0706,0.0086,0.0633,0.0553,0.0097,-0.0862,0.0272,-0.0072,-0.0758,0.0366,0.0303,-0.0011,-0.0068,-0.0473,-0.0871,-0.0196,-0.0324,-0.0114,-0.0282,-0.0997,-0.0486,-0.0292,-0.0443,0.0990,0.0073,0.0184,-0.0183,0.0827,0.0093,-0.0477,-0.0663,-0.0033,0.0864,-0.0571,-0.1020,-0.0991,-0.0011,-0.0159,-0.0887,-0.0090,0.0830,-0.1021,0.0679,-0.0191,-0.0963,0.0991,0.0311,0.0143,-0.0990,0.0254,-0.0934,0.0400,0.0579,-0.0892,-0.0659,0.0293,0.0820,-0.0527,-0.0345,0.0879,-0.0854,0.0104,0.0798,-0.0781,0.0338,0.0031,-0.0999,-0.0313,0.0287,-0.0887,-0.0863,-0.0977,0.0355,-0.0075,0.0192,0.0047,-0.0957,0.0335,-0.1010,-0.0495,0.0045,0.0119,0.0024,0.0801,0.0810,-0.0140,-0.0859,0.0365,-0.0405,-0.0783,-0.0718,-0.0270,0.0084,-0.0557,-0.0340,-0.0585,-0.0365,0.0245,-0.0975,-0.0431,0.0600,-0.0711,-0.0654,-0.0294,0.0306,-0.0470,-0.0216,-0.0698,-0.0903,-0.0232,-0.0935,-0.0045,-0.0867,0.0186,-0.0915,-0.0307,-0.0248,0.0392,-0.0771,0.0899,0.0868,0.0479,-0.0444,-0.0755,0.1018,-0.0133,-0.0749,-0.0374,-0.0156,-0.0437,-0.0918,0.0258,-0.0374,-0.0425,-0.0862,-0.0873,0.0383,0.0427,0.0413,0.0773,-0.0406,0.0225,0.0957,-0.0780,0.0854,0.0351,0.0435,-0.0634,-0.0863,0.0173,0.0902,-0.0517,-0.0603,-0.0801,0.0084,0.0276,-0.0973,0.0165,-0.0639,-0.0017,0.0633,-0.0245,0.0467,0.0644,-0.0271,0.0091,0.0799,0.0183,0.0569,0.0287,-0.0131,-0.0819,-0.0969,-0.0861,-0.0413,-0.0577,0.0406,0.0758,0.0416,0.0985,0.0572,0.0636,-0.0634,0.0727,0.0904,-0.0384,0.0394,-0.0284,-0.0776,-0.0734,0.0875,0.0992,-0.0810,-0.0502,0.0006,0.0577,0.0379,-0.0382,-0.0181,0.0848,-0.0655,-0.0865,0.0323,0.0610,-0.0888,0.0986,0.0454,0.0167,0.0570,-0.0809,0.0349,-0.0633,-0.0827,0.0558,-0.0316,-0.0899,0.0609,0.0780,-0.0599,-0.0847,0.0804,0.0945,0.0689,-0.0922,-0.0868,0.0344,-0.0597,-0.0284,-0.0833,-0.0218,-0.0079,0.0800,-0.0930,-0.0270,0.0852,-0.0716,-0.0976,-0.0283,-0.0729,0.0195,-0.0074,-0.0884,0.0429,0.0103,0.0937,0.0320,0.0671,0.0988,-0.0289,0.0257,-0.0483,0.0536,-0.0127,-0.0848,0.0409,0.0267,-0.0264,-0.0175,0.0049,-0.0640,0.0136,0.0453,0.0431,0.0149,-0.0428,0.0463,-0.0431,0.0475,0.0948,0.0897,0.0263,-0.0248,0.0444,0.0231,0.0358,-0.0046,-0.0547,0.0426,0.0202,0.0427,-0.0101,-0.0642,-0.0084,0.0510,-0.0157,0.0052,-0.0115,-0.0847,0.0491,-0.0791,-0.0632,0.0984,0.0275,-0.0939,-0.0936,0.0386,0.0880,0.0410,0.0212,0.0798,0.0865,-0.0917,-0.0442,0.0244,-0.0022,0.0504,0.0692,0.0876,0.0315,0.0417,-0.0060,0.0700,0.0868,-0.0659,-0.0682,-0.0541,-0.0011,0.0815,0.0696,0.0175,0.0181,-0.0446,-0.0710,0.0385,0.0186,0.0203,0.0251,0.0435,0.0324,0.0091,-0.0594,-0.0100,0.0026,0.0878,-0.1003,-0.0327,0.0300,0.0128,-0.0379,-0.0574,0.0476,-0.0160,-0.0826,0.0358,-0.0613,0.0193,-0.0770,0.0399,-0.0529,0.0160,-0.0368,0.0121,0.0318,0.0026,-0.0145,-0.0354,0.0899,0.0415,0.0576,-0.0236,0.0587,-0.0722,-0.0406,0.0292,-0.0512,-0.0480,-0.0827,0.0093,-0.0464,0.0730,0.0103,0.0952,0.0836,0.0722,-0.0624,-0.0304,-0.0426,0.0737,-0.0052,-0.0481,0.0842,-0.0935,0.0930,0.0629,0.0943,0.0073,-0.0099,-0.0205,-0.0085,-0.0642,0.0100,-0.0397,-0.0149,0.0666,0.0931,-0.0914,0.0639,0.0303,0.0208,0.0678,0.0092,0.0366,0.0663,-0.0363,0.0720,-0.0230,0.0548,-0.0022,-0.0359,0.0949,-0.0414,0.0187,0.0631,0.0783,-0.0223,-0.0690,-0.0595,0.0826,-0.0359,-0.0108,0.0125,-0.0289,0.0521,-0.0463,0.0401,-0.0068,0.0493,0.0201,0.0416,-0.0830,0.0291,-0.0655,0.0051,-0.0349,0.1021,-0.0355,-0.0403,-0.0785,-0.0922,-0.0904,-0.0699,-0.0571,-0.0874,0.0268,0.0089,-0.0957,-0.0833,-0.0374,0.0080,0.0681,-0.0325,0.0804,-0.0917,0.0061,-0.0849,-0.0911,-0.0554,-0.0655,0.0101,-0.0574,-0.0151,0.0223,-0.0452,-0.0506,0.0961,-0.0214,-0.0908,0.0184,0.0241,0.0527,0.0722,0.0750,0.0759,0.0053,0.0443,-0.0736,0.0332,0.1013,0.0168,-0.0606,-0.0179,0.0587,0.0887,0.0959,-0.0212,-0.0122,0.0081,-0.0151,0.0804,0.0730,0.0045,-0.0958,-0.0099,-0.0287,0.0659,-0.0030,0.0405,0.0061,0.0462,-0.0822,-0.0542,0.0221,0.0482,0.0955,0.0854,0.0707,-0.0381,-0.0674,0.0616,-0.0606,-0.0013,-0.0441,0.1005,0.0958,-0.0456,-0.0914,-0.0416,0.0482,-0.0566,0.0052,-0.0691,-0.0873,0.0271,0.0557,0.0103,-0.0420,-0.0580,-0.0421,0.0248,0.0771,-0.0989,0.0318,0.0847,-0.0916,0.0829,-0.0372,0.0031,-0.0076,0.0466,0.0493,0.0964,0.0171,-0.0041,-0.0198,-0.0544,0.1002,-0.0055,-0.0537,0.0427,-0.0608,0.0665,0.0423,-0.0422,-0.0262,-0.0171,-0.0305,-0.0037,0.0075,0.0575,-0.0539,0.0717,0.0970,0.0188,0.0891,0.0698,-0.0506,0.0324,-0.0066,-0.0019,0.0658,-0.0748,-0.0588,-0.0283,0.0975,0.0249,0.0806,-0.0834,-0.0794,-0.0918,0.0581,0.0399,0.0616,-0.0838,0.0972,-0.0731,0.0354,-0.0118,0.0671,0.0608,-0.0403,-0.0126,-0.0366,-0.0700,-0.0136,0.0537,-0.0476,0.0387,-0.0297,0.0032,
											  -0.0433,-0.0632,-0.0728,0.0134,-0.0147,-0.0206,-0.0741,0.0247,-0.0791,-0.0304,0.0748,-0.0310,-0.0491,-0.0362,-0.0895,0.0510,-0.0486,0.0510,-0.0970,-0.0737,-0.0157,0.0433,0.0195,-0.0807,-0.0158,-0.0437,-0.0880,-0.0335,-0.0539,-0.0841,0.0129,-0.0959,0.0665,0.0693,0.0614,-0.0273,0.0851,-0.0717,-0.0389,-0.0152,0.0050,0.0653,0.0231,0.0343,0.1010,-0.0608,-0.0933,-0.0653,-0.0603,0.0260,0.0163,-0.0248,0.0885,0.0024,-0.0628,0.0991,-0.0800,0.0202,-0.0900,-0.0387,-0.0150,-0.0623,-0.0332,0.0842,-0.0539,-0.0993,-0.0597,0.0454,0.0882,0.0487,0.0366,-0.0184,-0.0714,0.0745,-0.0306,0.0514,-0.0722,0.0078,-0.0669,-0.0681,0.0497,-0.0425,0.0966,-0.0928,-0.0340,-0.0296,0.0127,0.0397,-0.0488,-0.0086,0.0587,0.0828,-0.0019,-0.0043,0.0506,0.0464,-0.0014,0.0963,0.0737,-0.0029,0.0122,0.1018,-0.0573,-0.1005,0.0356,0.0660,0.0585,-0.0065,-0.0942,-0.0143,-0.0554,0.0294,0.0233,0.0924,-0.0864,-0.0888,-0.0552,0.0350,0.0638,-0.0475,0.0134,0.0530,0.0018,-0.0383,0.0938,0.0881,0.0497,-0.0392,-0.0253,0.0776,-0.0533,-0.0545,0.0159,-0.0258,0.0644,-0.0338,-0.0668,-0.0437,0.0373,0.0295,-0.0696,0.0301,0.0876,0.0257,-0.0446,-0.0870,-0.0662,-0.0807,0.0768,-0.0521,-0.0326,-0.0961,-0.0682,-0.0682,0.0073,0.0589,-0.0071,-0.0793,0.0350,-0.0393,0.1016,0.0888,-0.0483,0.0497,-0.0664,-0.0230,0.0513,-0.0395,0.0672,0.0004,0.0192,-0.0757,0.0180,0.0893,-0.0303,-0.0124,0.0168,0.0799,0.0132,0.0275,-0.0843,-0.0084,-0.0194,0.0004,-0.0037,0.0905,0.0538,0.0123,0.0576,-0.0245,0.0571,0.0028,-0.0513,-0.0511,-0.0655,-0.0350,-0.0059,0.0162,-0.0152,-0.0375,0.0984,-0.0216,-0.0659,0.0339,-0.0712,-0.0541,0.0689,-0.0230,-0.1019,0.0256,0.0984,0.0425,0.0111,0.0051,-0.0250,0.0923,-0.0565,-0.0958,-0.0306,-0.0139,0.0784,0.0580,-0.0575,-0.0689,-0.0903,0.0799,0.0623,0.0355,0.0719,-0.0966,-0.0018,0.0811,0.0497,0.0335,-0.0473,0.0525,-0.0532,0.0179,0.0221,0.0961,-0.0506,-0.0556,0.0746,0.0601,-0.0771,-0.0180,-0.0034,-0.0356,0.0561,0.0986,-0.0823,-0.0381,-0.0091,0.0771,-0.0003,-0.0983,-0.0616,-0.0818,0.0647,0.0911,-0.0349,0.0641,-0.0500,0.0390,-0.0149,-0.0536,-0.0359,0.0770,-0.0620,0.0077,-0.1007,0.0356,0.0349,0.0110,-0.0958,0.0677,0.0784,0.0803,0.0505,-0.0224,-0.0645,0.0624,0.0828,-0.0503,0.0532,0.0530,0.1002,0.0396,-0.0641,0.0443,0.0277,0.0901,0.0995,-0.0185,0.0900,-0.0010,-0.0083,0.0455,-0.0048,-0.0450,0.0768,-0.0394,-0.0673,-0.0582,0.0306,0.1016,0.0890,-0.0364,-0.0997,-0.0317,-0.0085,0.0011,-0.0302,-0.0530,0.0473,0.0751,-0.0607,0.0209,0.0362,-0.0078,-0.0276,-0.0800,-0.0948,0.0128,-0.0162,-0.0839,0.0249,-0.0163,-0.0889,0.0745,0.0483,0.0187,0.0189,0.0771,0.0398,-0.0797,0.0417,-0.0878,-0.0927,-0.0963,-0.0038,0.0135,0.0895,-0.0262,0.0631,0.0689,0.0271,0.0864,0.0738,-0.0092,0.0780,0.1016,0.1013,0.0827,-0.0264,-0.0897,0.0355,0.0563,0.0658,0.0571,0.0767,0.0667,0.0183,-0.0800,0.0637,0.0316,0.0336,-0.0810,-0.0884,-0.0784,-0.0291,0.0445,-0.0378,-0.0838,0.0321,-0.0156,-0.0597,0.0765,0.0362,0.0100,-0.0977,0.0837,0.0305,-0.0127,-0.1018,-0.0006,0.0051,0.0533,-0.0883,-0.0162,-0.0619,-0.0482,-0.0626,0.0212,-0.0276,0.0748,-0.0669,0.0819,0.0061,-0.0873,-0.0390,0.0791,-0.0014,-0.0205,-0.0142,0.0466,0.0816,0.0765,0.0255,-0.0275,-0.0791,-0.0392,0.0278,-0.0489,0.0817,-0.0756,0.0927,0.0752,-0.0211,0.0595,0.0826,-0.0636,-0.0299,0.0437,-0.0406,0.0550,0.0680,-0.0537,-0.0593,-0.0442,0.0352,-0.0105,-0.0617,-0.0834,0.0915,-0.0153,0.0294,0.0288,-0.0735,0.0146,0.0158,-0.0964,-0.0126,0.0433,-0.0446,0.0239,0.0792,0.0779,0.0039,-0.0588,-0.0833,-0.0394,0.0299,0.0485,0.0463,0.0142,-0.0336,-0.0316,0.0175,-0.0972,-0.0273,-0.0459,0.0131,-0.0393,0.0566,-0.0495,0.0136,0.0962,-0.0532,-0.0949,0.0836,-0.0273,-0.0082,0.0136,-0.0344,0.0886,-0.0806,-0.0979,0.0437,0.0345,0.0229,-0.0310,-0.0376,-0.0110,0.0008,0.0490,-0.0415,0.0305,-0.0600,-0.0968,-0.0628,-0.0630,-0.0340,0.0533,0.0583,0.0494,-0.0822,0.0308,-0.0957,0.0217,-0.0944,-0.0315,0.0720,0.0648,-0.0076,0.0432,-0.0338,0.0998,-0.0523,-0.0622,-0.0497,0.0298,0.0411,-0.0916,-0.0091,-0.0452,-0.0220,-0.0233,-0.0537,0.0955,0.0902,-0.0650,0.0964,0.0987,0.0148,0.0819,0.0866,-0.0637,-0.0809,-0.0055,-0.0214,-0.0303,0.0532,-0.0811,0.0493,-0.0242,-0.0672,-0.1016,0.0171,-0.0963,-0.0094,0.0469,-0.0814,0.0665,0.1000,-0.0674,-0.0971,-0.0023,0.0765,-0.0801,0.0336,-0.0759,-0.1004,0.0939,0.0461,0.0418,-0.0402,0.0540,-0.0529,-0.0647,0.0559,0.0106,-0.0249,0.0478,0.0253,0.0364,-0.0593,0.0002,0.0025,0.0416,-0.0441,0.0927,-0.0472,-0.0298,-0.0683,0.0573,0.0019,-0.0007,0.0141,-0.0073,-0.0559,0.0813,-0.0776,-0.0904,-0.0511,-0.0215,-0.0262,0.0528,0.0884,-0.0646,-0.0933,-0.0702,-0.0958,-0.0674,-0.0545,0.0373,-0.0377,-0.0930,-0.0964,0.0918,0.0096,-0.0182,0.0277,-0.0932,0.0891,-0.0840,0.0189,0.0094,0.0409,0.0749,0.0652,0.0133,-0.0277,-0.0134,-0.0193,-0.0646,0.0762,0.0085,0.0487,0.0350,0.0760,0.0798,0.0638,-0.0671,-0.0592,0.0942,-0.0520,0.0724,0.0665,-0.0917,0.0443,0.0059,-0.0863,-0.0505,0.0735,0.0376,-0.0013,-0.0623,0.0531,-0.0226,0.0661,-0.0354,-0.0115,0.0077,0.0271,-0.0363,-0.0687,0.0955,0.0855,0.0166,-0.0688,-0.0285,-0.0336,0.0815,-0.0247,0.0456,-0.0618,0.0816,-0.0705,-0.0011,-0.0306,0.0574,0.0887,-0.0366,-0.0938,0.0317,-0.0888,-0.0365,0.0310,-0.0262,-0.0515,-0.0594,-0.0289,-0.0944,0.0251,0.0130,0.0847,0.0369,0.0709,-0.1003,-0.0436,-0.0115,0.0896,-0.0468,0.0775,0.0220,0.0340,0.0450,-0.0302,-0.0201,-0.0435,-0.0982,0.0022,0.0804,-0.0937,-0.1020,0.0894,-0.0687,0.0674,-0.0271,0.0073,0.0697,0.0278,-0.1000,0.0568,-0.0039,0.0397,-0.0089,0.0816,-0.0484,0.0275,-0.0310,0.1016,0.0289,-0.0559,0.0177,-0.0091,-0.0918,0.0905,0.0725,-0.0049,-0.0542,0.0371,-0.0575,-0.0631,0.0050,-0.0416,-0.0602,-0.0263,0.0417,0.0976,0.0307,0.0326,-0.0312,0.0317,0.0967,0.0124,-0.0450,-0.0997,-0.0038,0.0006,-0.0497,-0.0456,-0.0259,-0.0074,0.0591,-0.0950,-0.0766,0.0275,0.0345,0.0676,-0.0160,-0.0306,0.0720,-0.0932,0.0745,0.0603,0.0950,0.0039};
	float bias_linea2_fw[Q] = {0.0951,-0.0582,0.0355,0.0408,-0.0230,0.0267,-0.0757,0.0872,0.0133,-0.0916,-0.0074,-0.0857,0.0348,-0.0029,0.0068,-0.0737,0.0046,-0.0707,-0.0415,-0.0356,0.0332,0.0424,0.0363,-0.0179};
	float matrix_weights_linear2_fw_t[K][Q] = {};
	float output_layer1_fw[M][K] = {};

// Print Inputs
	printf("\n==================================Inputs==================================: \n");
//		printf("\nValues3 matrix: \n");
//		 for (int m = 0; m<M; ++m) {
//			  for (int n = 0; n<N; ++n) {
//				  printf("%f  ", Values3[m][n]);
//				  if (n == N - 1)
//					  printf("\n");
//			  }
//		  }
//			printf("\nValues4 matrix: \n");
//			 for (int m = 0; m<M; ++m) {
//				  for (int n = 0; n<N; ++n) {
//					  printf("%f  ", Values4[m][n]);
//					  if (n == N - 1)
//						  printf("\n");
//				  }
//			  }
// Show all the Values for the 8 heads
	    for(int m = 0; m < M; m++) {
	       for(int n = 0; n < N; n++) {
	           // To store elements of Values1
	    	   total_Values[m][n] = Values1[m][n];
	           // To store elements of Values2
	    	   total_Values[m][n + N] = Values2[m][n];
	           // To store elements of Values3
	    	   total_Values[m][n + N*2] = Values3[m][n];
	           // To store elements of Values4
	    	   total_Values[m][n + N*3] = Values4[m][n];
	           // To store elements of Values5
	    	   total_Values[m][n + N*4] = Values5[m][n];
	           // To store elements of Values6
	    	   total_Values[m][n + N*5] = Values6[m][n];
	           // To store elements of Values7
	    	   total_Values[m][n + N*6] = Values7[m][n];
	           // To store elements of Values8
	    	   total_Values[m][n + N*7] = Values8[m][n];
	       }
	   }

	printf("\nComplete Values matrix: \n");
	printf("\n           Values1                          Values2                         Values3                         Values4                         Values5                         Values6                         Values7                         Values8 \n");
		for (int m = 0; m<M; ++m) {
			for ( int n= 0; n<N*8; ++n) {
				printf("%f  ", total_Values[m][n]);
				if (n == N - 1 || n == N*2 -1 || n == N*3 -1 || n == N*4 -1 || n == N*5 -1 || n == N*6 -1 || n == N*7 -1)
					 printf("  ");
				if (n == N*8 - 1)
					 printf("\n");
			}
		 }

// Show all the Keys for the 8 heads
//	printf("\nKeys1 matrix: \n");
//	  for (int a = 0; a<A; ++a) {
//		  for (int b = 0; b<B; ++b) {
//			  printf("%f  ", Keys1[a][b]);
//			  if (b == B - 1)
//				  printf("\n");
//		  }
//	  }
//	printf("\nKeys2 matrix: \n");
//	  for (int a = 0; a<A; ++a) {
//		  for (int b = 0; b<B; ++b) {
//			  printf("%f  ", Keys2[a][b]);
//			  if (b == B - 1)
//				  printf("\n");
//		  }
//	  }
	    for(int m = 0; m < M; m++) {
	       for(int n = 0; n < N; n++) {
	    	   total_Keys[m][n] = Keys1[m][n];
	    	   total_Keys[m][n + N] = Keys2[m][n];
	    	   total_Keys[m][n + N*2] = Keys3[m][n];
	    	   total_Keys[m][n + N*3] = Keys4[m][n];
	    	   total_Keys[m][n + N*4] = Keys5[m][n];
	    	   total_Keys[m][n + N*5] = Keys6[m][n];
	    	   total_Keys[m][n + N*6] = Keys7[m][n];
	    	   total_Keys[m][n + N*7] = Keys8[m][n];
	       }
	   }

	printf("\nComplete Keys matrix: \n");
	printf("\n             Keys1                            Keys2                            Keys3                            Keys4                            Keys5                            Keys6                            Keys7                            Keys8 \n");
		for (int m = 0; m<M; ++m) {
			for ( int n= 0; n<N*8; ++n) {
				printf("%f  ", total_Keys[m][n]);
				if (n == N - 1 || n == N*2 -1 || n == N*3 -1 || n == N*4 -1 || n == N*5 -1 || n == N*6 -1 || n == N*7 -1)
					 printf("  ");
				if (n == N*8 - 1)
					 printf("\n");
			}
		 }

// Show all the Queries for the 8 heads
//	printf("\nQueries1 matrix: \n");
//		for (int m = 0; m<M; ++m) {
//			for (int n = 0; n<N; ++n) {
//				printf("%f  ", Queries1[m][n]);
//				if (n == N - 1)
//					printf("\n");
//		  }
//	  }
	    for(int m = 0; m < M; m++) {
	       for(int n = 0; n < N; n++) {
	    	   total_Queries[m][n] = Queries1[m][n];
	    	   total_Queries[m][n + N] = Queries2[m][n];
	    	   total_Queries[m][n + N*2] = Queries3[m][n];
	    	   total_Queries[m][n + N*3] = Queries4[m][n];
	    	   total_Queries[m][n + N*4] = Queries5[m][n];
	    	   total_Queries[m][n + N*5] = Queries6[m][n];
	    	   total_Queries[m][n + N*6] = Queries7[m][n];
	    	   total_Queries[m][n + N*7] = Queries8[m][n];
	       }
	   }

	printf("\nComplete Queries matrix: \n");
	printf("\n            Queries1                         Queries2                         Queries3                         Queries4                         Queries5                         Queries6                         Queries7                         Queries8 \n");
		for (int m = 0; m<M; ++m) {
			for ( int n= 0; n<N*8; ++n) {
				printf("%f  ", total_Queries[m][n]);
				if (n == N - 1 || n == N*2 -1 || n == N*3 -1 || n == N*4 -1 || n == N*5 -1 || n == N*6 -1 || n == N*7 -1)
					 printf("  ");
				if (n == N*8 - 1)
					 printf("\n");
			}
		 }

// Keys Transpose Matrix
	printf("\n==================================Matrix Transpose==================================: \n");

	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t1[n][m] = Keys1[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t2[n][m] = Keys2[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t3[n][m] = Keys3[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t4[n][m] = Keys4[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t5[n][m] = Keys5[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t6[n][m] = Keys6[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t7[n][m] = Keys7[m][n];

		}
	}
	for (int m = 0; m<M; ++m) {
		for (int n = 0; n<N; ++n) {
			Keys_t8[n][m] = Keys8[m][n];

		}
	}

// Show all the Keys transpose for the 8 heads
//	printf("\nKeys_t1 matrix: \n");
//	  for (int b = 0; b<B; ++b) {
//		  for ( int a= 0; a<A; ++a) {
//			  printf("%f  ", Keys_t1[b][a]);
//			  if (a == A - 1)
//				  printf("\n");
//		  }
//	  }
//	printf("\nKeys_t2 matrix: \n");
//	  for (int b = 0; b<B; ++b) {
//		  for ( int a= 0; a<A; ++a) {
//			  printf("%f  ", Keys_t2[b][a]);
//			  if (a == A - 1)
//				 printf("\n");
//			  }
//	  }
	    for(int n = 0; n < N; n++) {
	       for(int m = 0; m < M; m++) {
	    	   total_Keys_transpose[n][m] = Keys_t1[n][m];
	    	   total_Keys_transpose[n][m + M] = Keys_t2[n][m];
	    	   total_Keys_transpose[n][m + M*2] = Keys_t3[n][m];
	    	   total_Keys_transpose[n][m + M*3] = Keys_t4[n][m];
	    	   total_Keys_transpose[n][m + M*4] = Keys_t5[n][m];
	    	   total_Keys_transpose[n][m + M*5] = Keys_t6[n][m];
	    	   total_Keys_transpose[n][m + M*6] = Keys_t7[n][m];
	    	   total_Keys_transpose[n][m + M*7] = Keys_t8[n][m];
	       }
	   }

	printf("\nComplete Keys transpose matrix: \n");
	printf("\n           Keys_t1                             Keys_t2                              Keys_t3                           Keys_t4                            Keys_t5                             Keys_t6                            Keys_t7                            Keys_t8 \n");
		for (int n = 0; n<N; ++n) {
			for ( int m= 0; m<M*8; ++m) {
				printf("%f  ", total_Keys_transpose[n][m]);
				if (m == M - 1 || m == M*2 -1 || m == M*3 -1 || m == M*4 -1 || m == M*5 -1 || m == M*6 -1 || m == M*7 -1)
					 printf("    ");
				if (m == M*8 - 1)
					 printf("\n");
			}
		 }



// Send data to the FPGA
	// Send Queries for each head
	// Send Keys_t for each head

// Call FPGA
	score_matrix(score_m_h1, Queries1, Keys_t1);
	score_matrix(score_m_h2, Queries2, Keys_t2);
	score_matrix(score_m_h3, Queries3, Keys_t3);
	score_matrix(score_m_h4, Queries4, Keys_t4);
	score_matrix(score_m_h5, Queries5, Keys_t5);
	score_matrix(score_m_h6, Queries6, Keys_t6);
	score_matrix(score_m_h7, Queries7, Keys_t7);
	score_matrix(score_m_h8, Queries8, Keys_t8);

//	// Read data from the FPGA
//		// receive score_m_hx for each head


	printf("\n==================================Score Matrix==================================: \n");
//	printf("\nScore Matrix: \n");
//	  for (int m = 0; m<M; ++m) {
//		  for (int o = 0; o<O; ++o) {
//			  printf("QK[%i][%i] %f ", m, o, score_m_h1[m][o]);
//			  if (o == O - 1)
//				  printf("\n");
//		  }
//	  }
//		printf("\nScore Matrix: \n");
//		  for (int m = 0; m<M; ++m) {
//			  for (int o = 0; o<O; ++o) {
//				  printf("QK[%i][%i] %f ", m, o, score_m_h2[m][o]);
//				  if (o == O - 1)
//					  printf("\n");
//			  }
//		  }
// Show all score matrices for the 8 heads
		for(int m = 0; m < M; m++) {
		       for(int o = 0; o < O; o++) {
		    	   total_score_m[m][o] = score_m_h1[m][o];
		    	   total_score_m[m][o + O] = score_m_h2[m][o];
		    	   total_score_m[m][o + O*2] = score_m_h3[m][o];
		    	   total_score_m[m][o + O*3] = score_m_h4[m][o];
		    	   total_score_m[m][o + O*4] = score_m_h5[m][o];
		    	   total_score_m[m][o + O*5] = score_m_h6[m][o];
		    	   total_score_m[m][o + O*6] = score_m_h7[m][o];
		    	   total_score_m[m][o + O*7] = score_m_h8[m][o];
		       }
		 }

		printf("\nComplete Score matrix: \n");
		printf("\n            score_m_h1                        score_m_h2                        score_m_h3                       score_m_h4                       score_m_h5                       score_m_h6                       score_m_h7                       score_m_h8 \n");
			for (int m = 0; m<M; ++m) {
				for ( int o= 0; o<O*8; ++o) {
					printf("%f  ", total_score_m[m][o]);
					if (o == O - 1 || o == O*2 -1 || o == O*3 -1 || o == O*4 -1 || o == O*5 -1 || o == O*6 -1 || o == O*7 -1)
						 printf("  ");
					if (o == O*8 - 1)
						 printf("\n");
				}
			 }

	printf("\n==================================Score Matrix/sqrt(emb size)==================================: \n");
// Score Matrix / Square Root (dim)
	sqrt_emb_size = sqrt(emb_size);
			for (int m = 0; m <M; m++) {
				for (int o = 0; o <O; o++) {
					score_m_h1_div[m][o] = score_m_h1[m][o]/sqrt_emb_size;
					score_m_h2_div[m][o] = score_m_h2[m][o]/sqrt_emb_size;
					score_m_h3_div[m][o] = score_m_h3[m][o]/sqrt_emb_size;
					score_m_h4_div[m][o] = score_m_h4[m][o]/sqrt_emb_size;
					score_m_h5_div[m][o] = score_m_h5[m][o]/sqrt_emb_size;
					score_m_h6_div[m][o] = score_m_h6[m][o]/sqrt_emb_size;
					score_m_h7_div[m][o] = score_m_h7[m][o]/sqrt_emb_size;
					score_m_h8_div[m][o] = score_m_h8[m][o]/sqrt_emb_size;
				}
			}
// Show all score matrices for the 8 heads
			for(int m = 0; m < M; m++) {
			       for(int o = 0; o < O; o++) {
			    	   total_score_m_div[m][o] = score_m_h1_div[m][o];
			    	   total_score_m_div[m][o + O] = score_m_h2_div[m][o];
			    	   total_score_m_div[m][o + O*2] = score_m_h3_div[m][o];
			    	   total_score_m_div[m][o + O*3] = score_m_h4_div[m][o];
			    	   total_score_m_div[m][o + O*4] = score_m_h5_div[m][o];
			    	   total_score_m_div[m][o + O*5] = score_m_h6_div[m][o];
			    	   total_score_m_div[m][o + O*6] = score_m_h7_div[m][o];
			    	   total_score_m_div[m][o + O*7] = score_m_h8_div[m][o];
			       }
			 }

			printf("\nComplete Score matrix: \n");
			printf("\n            score_m_h1                        score_m_h2                        score_m_h3                       score_m_h4                       score_m_h5                       score_m_h6                       score_m_h7                       score_m_h8 \n");
				for (int m = 0; m<M; ++m) {
					for ( int o= 0; o<O*8; ++o) {
						printf("%f  ", total_score_m_div[m][o]);
						if (o == O - 1 || o == O*2 -1 || o == O*3 -1 || o == O*4 -1 || o == O*5 -1 || o == O*6 -1 || o == O*7 -1)
							 printf("  ");
						if (o == O*8 - 1)
							 printf("\n");
					}
				 }

printf("\n==================================Sum of Rows exp(score matrix)==================================: \n");
// Sum of all exponentials for each row in each head
	for (int m = 0; m <M; m++) {
		for (int o = 0; o <O; o++) {
			sum_score_m_h1[m] += exp(score_m_h1_div[m][o]);
			sum_score_m_h2[m] += exp(score_m_h2_div[m][o]);
			sum_score_m_h3[m] += exp(score_m_h3_div[m][o]);
			sum_score_m_h4[m] += exp(score_m_h4_div[m][o]);
			sum_score_m_h5[m] += exp(score_m_h5_div[m][o]);
			sum_score_m_h6[m] += exp(score_m_h6_div[m][o]);
			sum_score_m_h7[m] += exp(score_m_h7_div[m][o]);
			sum_score_m_h8[m] += exp(score_m_h8_div[m][o]);
		 }

	}

	printf("\nHead 1: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h1[m]);
	}
	printf("\nHead 2: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h2[m]);
	}
	printf("\nHead 3: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h3[m]);
	}
	printf("\nHead 4: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h4[m]);
	}
	printf("\nHead 5: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h5[m]);
	}
	printf("\nHead 6: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h6[m]);
	}
	printf("\nHead 7: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h7[m]);
	}
	printf("\nHead 8: ");
	for (int m = 0; m < M; m++) {
		printf("%f ", sum_score_m_h8[m]);
	}
	printf("\n");

// Send all the score matrices to the FPGA AND sum_score_m_hx
  //send score_m_h1_div
  //send score_m_h2_div  ...

  //send sum_score_m_h1
  //send sum_score_m_h2 ...

// Call FPGA
   softmax_function(attention_h1, score_m_h1_div, sum_score_m_h1);
   softmax_function(attention_h2, score_m_h2_div, sum_score_m_h2);
   softmax_function(attention_h3, score_m_h3_div, sum_score_m_h3);
   softmax_function(attention_h4, score_m_h4_div, sum_score_m_h4);
   softmax_function(attention_h5, score_m_h5_div, sum_score_m_h5);
   softmax_function(attention_h6, score_m_h6_div, sum_score_m_h6);
   softmax_function(attention_h7, score_m_h7_div, sum_score_m_h7);
   softmax_function(attention_h8, score_m_h8_div, sum_score_m_h8);

// Receive from the FPGA
   // receive attention_h1
   // receive attention_h2 ...

	printf("\n==================================Attention Matrices==================================: \n");
// Show all attention matrices for the 8 heads
	for(int m = 0; m < M; m++) {
		for(int o = 0; o < O; o++) {
			total_attention_h[m][o] = attention_h1[m][o];
			total_attention_h[m][o + O] = attention_h2[m][o];
			total_attention_h[m][o + O*2] = attention_h3[m][o];
			total_attention_h[m][o + O*3] = attention_h4[m][o];
			total_attention_h[m][o + O*4] = attention_h5[m][o];
			total_attention_h[m][o + O*5] = attention_h6[m][o];
			total_attention_h[m][o + O*6] = attention_h7[m][o];
			total_attention_h[m][o + O*7] = attention_h8[m][o];
		}
	}

	printf("\nComplete Attention matrix: \n");
	printf("\n     Attention_matrix_h1             Attention_matrix_h2            Attention_matrix_h3              Attention_matrix_h4            Attention_matrix_h5             Attention_matrix_h6              Attention_matrix_h7              Attention_matrix_h8 \n");
	for (int m = 0; m<M; ++m) {
		for ( int o= 0; o<O*8; ++o) {
			printf("%f  ", total_attention_h[m][o]);
			if (o == O - 1 || o == O*2 -1 || o == O*3 -1 || o == O*4 -1 || o == O*5 -1 || o == O*6 -1 || o == O*7 -1)
				printf("  ");
			if (o == O*8 - 1)
				printf("\n");
		}
	}

///////////////////////////////////////VERIFICATION THAT THE ATTENTION MATRICES ARE CORRECT///////////////////////////////////////
	for(int m = 0; m < M; m++) {
		for(int o = 0; o < O; o++) {
			total_attention_h_pytorch[m][o] = attention_h1_pytorch[m][o];
			total_attention_h_pytorch[m][o + O] = attention_h2_pytorch[m][o];
			total_attention_h_pytorch[m][o + O*2] = attention_h3_pytorch[m][o];
			total_attention_h_pytorch[m][o + O*3] = attention_h4_pytorch[m][o];
			total_attention_h_pytorch[m][o + O*4] = attention_h5_pytorch[m][o];
			total_attention_h_pytorch[m][o + O*5] = attention_h6_pytorch[m][o];
			total_attention_h_pytorch[m][o + O*6] = attention_h7_pytorch[m][o];
			total_attention_h_pytorch[m][o + O*7] = attention_h8_pytorch[m][o];
		}
	}
//// Just to verify that is printed properly
//	printf("\nComplete Attention matrix from Pytorch: \n");
//	printf("\n     Attention_matrix_h1             Attention_matrix_h2            Attention_matrix_h3              Attention_matrix_h4            Attention_matrix_h5             Attention_matrix_h6              Attention_matrix_h7              Attention_matrix_h8 \n");
//	for (int m = 0; m<M; ++m) {
//		for ( int o= 0; o<O*8; ++o) {
//			printf("%f  ", total_attention_h_pytorch[m][o]);
//			if (o == O - 1 || o == O*2 -1 || o == O*3 -1 || o == O*4 -1 || o == O*5 -1 || o == O*6 -1 || o == O*7 -1)
//				printf("  ");
//			if (o == O*8 - 1)
//				printf("\n");
//		}
//	}
	printf("\n Attention Matrix has an error of: %f", attention_check(total_attention_h_pytorch, total_attention_h));
	if (attention_check(total_attention_h_pytorch, total_attention_h) > 0.000001) { printf("\nThe attention matrix is correct!\n"); }
	else {
		printf("Error\n"); // If there is an error adjust attention:
		printf("\nAdjusting Values..: \n");
		  for (int m = 0; m<M; ++m) {
			  for (int o = 0; o<O; ++o) {
					attention_h1[m][o] = attention_h1_pytorch[m][o];
					attention_h2[m][o] = attention_h1_pytorch[m][o];
					attention_h3[m][o] = attention_h1_pytorch[m][o];
					attention_h4[m][o] = attention_h1_pytorch[m][o];
					attention_h5[m][o] = attention_h1_pytorch[m][o];
					attention_h6[m][o] = attention_h1_pytorch[m][o];
					attention_h7[m][o] = attention_h1_pytorch[m][o];
					attention_h8[m][o] = attention_h1_pytorch[m][o];

			  }
		  }
		  printf("New values adjusted!\n");
	}

// The results are correct or adjusted. Therefore, we send the attention matrices to calculate the output (Attention * Values)

// Send all the attention matrices to the FPGA AND the Values
	  //send attention_h1
	  //send attention_h2  ...

	  //send Values1
	  //send Values2 ...

// Call FPGA
	output_matrix(output_h1, attention_h1, Values1);
	output_matrix(output_h2, attention_h2, Values2);
	output_matrix(output_h3, attention_h3, Values3);
	output_matrix(output_h4, attention_h4, Values4);
	output_matrix(output_h5, attention_h5, Values5);
	output_matrix(output_h6, attention_h6, Values6);
	output_matrix(output_h7, attention_h7, Values7);
	output_matrix(output_h8, attention_h8, Values8);


// Receive values of output_hx
//	 Read from port

	printf("\n==================================Concatenation Output Matrices==================================: \n");
// Show all output matrices for the 8 heads
//		printf("\nOtput Matrix h1: \n");
//		  for (int m = 0; m<M; ++m) {
//			  for (int o = 0; o<O; ++o) {
//				  printf(" %f", output_h1[m][o]);
//				  if (o == O - 1)
//					  printf("\n");
//			  }
//		  }
		    for(int m = 0; m < M; m++) {
		       for(int n = 0; n < N; n++) {
		    	   concatenated_total_output_matrix[m][n] = output_h1[m][n];
		    	   concatenated_total_output_matrix[m][n + N] = output_h2[m][n];
		    	   concatenated_total_output_matrix[m][n + N*2] = output_h3[m][n];
		    	   concatenated_total_output_matrix[m][n + N*3] = output_h4[m][n];
		    	   concatenated_total_output_matrix[m][n + N*4] = output_h5[m][n];
		    	   concatenated_total_output_matrix[m][n + N*5] = output_h6[m][n];
		    	   concatenated_total_output_matrix[m][n + N*6] = output_h7[m][n];
		    	   concatenated_total_output_matrix[m][n + N*7] = output_h8[m][n];
		       }
		   }

		printf("\nConcatenated Output matrix: \n");
		printf("\n           Output1                          Output2                         Output3                          Output4                           Output5                           Output6                           Output7                           Output8 \n");
			for (int m = 0; m<M; ++m) {
				for ( int n= 0; n<N*8; ++n) {
					printf("%f  ", concatenated_total_output_matrix[m][n]);
					if (n == N - 1 || n == N*2 -1 || n == N*3 -1 || n == N*4 -1 || n == N*5 -1 || n == N*6 -1 || n == N*7 -1)
						 printf("  ");
					if (n == N*8 - 1)
						 printf("\n");
				}
			 }

		printf("\n==================================Output Linear Layer [OUTPUT MULTI-HEAD ATTENTION LAYER]==================================: \n");
// Show the output of the linear layer [3x24][24x24]T + bias = [3x24]
	// Transpose of the weights matrices
		for (int p = 0; p<P; ++p) {
			for (int q = 0; q<Q; ++q) {
				matrix_weights_linear1_t[q][p] = matrix_weights_linear1[p][q];
			}
		}
//	// Print Matrix W. and WT. for verification
//		printf("\nWeights Linear Layer 1: \n");
//			 for (int p = 0; p<P; ++p) {
//				 for ( int q= 0; q<Q; ++q) {
//					 printf("%f  ", matrix_weights_linear1[p][q]);
//					 if (q == Q - 1)
//					    printf("\n");
//				}
//			 }
//		printf("\nWeights T. Linear Layer 1: \n");
//			 for ( int q= 0; q<Q; ++q) {
//			    for ( int p = 0; p<P; ++p) {
//					 printf("%f  ", matrix_weights_linear1_t[q][p]);
//					 if (p == P - 1)
//						 printf("\n");
//				}
//			 }

// Send to the FPGA concatenated_total_output_matrix[3][24] and weights T.
	// Send bias [1x24]

//Call FPGA
output_matrix_linear(concatenated_output_matrix_linear, concatenated_total_output_matrix, matrix_weights_linear1_t, bias_linear);

// Read data from port

// Show output matrix Linear Layer
		printf("\n           Output1L                        Output2L                       Output3L                      Output4L                       Output5L                       Output6L                         Output7L                       Output8L \n");
	         for ( int m= 0; m<M; ++m) {
			    for ( int p = 0; p<P; ++p) {
					 printf("%f  ", concatenated_output_matrix_linear[m][p]);
					 if (p == P - 1)
						 printf("\n");
				}
			 }

///////////////////////////////////////VERIFICATION OF THE MULTI-HEAD ATT. LAYER///////////////////////////////////////
	printf("\nOutput Linear Layer has an error of: %f", output_attention_check(concatenated_output_matrix_linear, concatenated_output_matrix_linear_pytorch));
	  if (output_attention_check(concatenated_output_matrix_linear, concatenated_output_matrix_linear_pytorch) > -0.000007) { printf("\nThe Attention Layer is Correct!\n"); }
	     else {
	         printf("Error\n"); // If there is an error adjust attention:
	         printf("\nAdjusting Values..: \n");
	         		  for (int m = 0; m<M; ++m) {
	         			  for (int p = 0; p<P; ++p) {
	         				 concatenated_output_matrix_linear[m][p] = concatenated_output_matrix_linear_pytorch[m][p];
	         			  }
	         		  }
	         		  printf("New values adjusted!\n");
	         }

//////////////////////////////////////////////////////////////////////////////     Add        ////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////   & Norm Layer /////////////////////////////////////////////////////////////////////
	    printf("\n==================================Output linear layer + Q==================================: \n");
//// Show the output of the linear layer from pytorch just for verification
//		printf("\n              Output1L                          Output2L                         Output3L                          Output4L                           Output5L                           Output6L                           Output7L                           Output8L \n");
//			for (int m = 0; m<M; ++m) {
//				for (int p = 0; p<P; ++p) {
//					 printf("%f", concatenated_output_matrix_linear_pytorch[m][p]);
//					 printf("  ");
//					 if (p == P - 1)
//						printf("\n");
//				}
//			 }

// Directly sum concatenated_output_matrix_linear_pytorch + Queries
		 for (int m = 0; m<M; m++) {
			 for (int p = 0; p<P; p++) {
					sum_out_attention_queries[m][p] = concatenated_output_matrix_linear[m][p] + total_Queries[m][p];
			 }
		}

// Show Output linear layer + Q
	printf("\n              Att+Q                        Att+Q                        Att+Q                           Att+Q                          Att+Q                           Att+Q                          Att+Q                          Att+Q\n");
		for ( int m= 0; m<M; ++m) {
			for ( int p = 0; p<P; ++p) {
				 printf("%f  ", sum_out_attention_queries[m][p]);
				 if (p == P - 1)
					printf("\n");
			}
		}

	printf("\n==================================[OUTPUT NORMALIZATION LAYER]==================================: \n");
// Norm layer
		printf("\n            Norm1                       Norm2                      Norm3                            Norm4                          Norm5                          Norm6                          Norm7                    Norm8     \n");
			for ( int m= 0; m<M; ++m) {
				for ( int p = 0; p<P; ++p) {
					 printf("%f  ", norm_layer1[m][p]);
					 if (p == P - 1)
						printf("\n");
				}
		}

////////////////////////////////////////////////////////////////////////////////     Feed        ////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////   & Forward Layer //////////////////////////////////////////////////////////////////
   printf("\n==================================Feed Forward Layer==================================: \n");
// Show the output of the linear layer [3x24][24x96]T + bias = [3x96]
	// Transpose of the weights matrix
		for (int k = 0; k<Q; ++k) {
			for (int q = 0; q<Q; ++q) {
				matrix_weights_linear1_fw_t[q][k] = matrix_weights_linear1_fw[k][q];
			}
		}
//	// Print Matrix W. and WT. for verification
//	printf("\nWeights Linear Layer 1: \n");
//						 for (int k = 0; k<K; ++k) {
//							 for ( int q= 0; q<Q; ++q) {
//								 printf("%f  ", matrix_weights_linear1_fw[k][q]);
//								 if (q == Q - 1)
//								    printf("\n");
//							}
//						 }
//	printf("\nWeights Linear T. Layer 1: \n");
//						for (int q = 0; q<Q; ++q) {
//							for ( int k= 0; k<K; ++k) {
//								printf("%f  ", matrix_weights_linear1_fw_t[q][k]);
//								if (k == K - 1)
//									printf("\n");
//							}
//						}

// Send to the FPGA norm_layer1[3][24] and weights T.[24][96]
	// Send bias [1x96]

//Call FPGA
	output_matrix_linear1_fw(output_layer1_fw, norm_layer1, matrix_weights_linear1_fw_t, bias_linear1_fw);

// Read data from port

// Output first Linear Layer
	printf("\nOutput Linear Layer     \n");
		for ( int m= 0; m<M; ++m) {
			for ( int k = 0; k<K; ++k) {
				 printf("%f  ", output_layer1_fw[m][k]);
				 if (k == K - 1)
					printf("\n");
			}
	}

	printf("\n==================================[OUTPUT NORMALIZATION LAYER]==================================: \n");
// Norm layer
		printf("\n            Norm1                       Norm2                      Norm3                            Norm4                          Norm5                          Norm6                          Norm7                    Norm8     \n");
			for ( int m= 0; m<M; ++m) {
				for ( int p = 0; p<P; ++p) {
					 printf("%f  ", norm_layer2[m][p]);
					 if (p == P - 1)
						printf("\n");
				}
		}






}
