INFO: [SIM 2] *************** CSIM start ***************
INFO: [SIM 4] CSIM will launch GCC as the compiler.
   Compiling ../../../../transformer.cpp in debug mode
   Generating csim.exe

==================================Inputs==================================: 

Complete Values matrix: 

           Values1                          Values2                         Values3                         Values4                         Values5                         Values6                         Values7                         Values8 
0.832000  0.465500  -0.603800    0.418300  -0.323300  0.326500    0.148200  0.059200  -0.304700    2.011800  -0.400500  0.731500    0.961800  0.533100  -0.254900    -1.243500  0.278400  -1.224100    2.426600  -0.222000  1.840600    0.239900  -0.156100  0.066200  
-0.718900  0.708800  -1.134600    0.196300  0.241000  0.298900    0.079100  0.310200  0.252200    -0.613600  0.319200  -0.080300    -0.445000  0.541900  -0.703200    -0.954900  -0.154800  -0.049300    0.023000  -0.207600  0.829000    0.209100  -0.240800  0.147000  
-0.348100  1.269400  -1.722400    1.722600  -0.524000  1.143700    1.274700  -0.080300  0.551100    -0.061900  0.262100  0.323300    -0.813800  0.729200  -0.744000    1.378400  -0.119700  0.326800    -1.427400  0.323100  -2.517900    -1.060400  -1.181400  0.390800  

Complete Keys matrix: 

             Keys1                            Keys2                            Keys3                            Keys4                            Keys5                            Keys6                            Keys7                            Keys8 
0.400500  0.484200  -0.799300    -0.270900  -0.229800  -0.348500    -0.210700  0.264800  -0.518100    0.220500  -0.538400  -1.000200    0.970100  0.167500  -0.185500    -0.972200  1.012900  -0.628600    1.952800  -1.539700  0.883700    -0.233300  -0.028900  -0.351100  
0.127100  0.880000  -0.091000    0.895100  -0.297500  0.793900    0.976900  -0.269600  0.928300    0.527000  0.007200  0.903100    0.301400  0.529600  0.129000    -0.504500  0.044300  0.352400    0.521800  -0.702900  1.005500    -0.326000  -0.088300  -0.345000  
0.699000  1.324200  -0.345200    0.401100  -0.892100  -0.311500    0.618500  -0.445600  -0.171500    0.939300  -0.328600  1.066900    0.617100  0.528000  0.676200    0.282900  -0.238000  -0.699100    -2.409400  2.149300  -2.592900    -2.228900  -0.190600  -0.686100  

Complete Queries matrix: 

            Queries1                         Queries2                         Queries3                         Queries4                         Queries5                         Queries6                         Queries7                         Queries8 
-1.580400  -1.671900  -1.223000    0.310900  0.068500  -0.250000    -0.507700  -0.691900  -0.345800    -0.226100  -0.653100  -1.659600    -1.283500  -1.079700  -1.130300    -0.820400  -1.070500  0.533400    0.738200  1.207700  -1.369800    0.012900  -0.178400  -0.218900  
-1.326100  -1.140700  0.073100    0.086200  0.530700  0.003300    0.066700  0.586200  0.100600    0.124100  0.584000  0.611400    -0.884300  -0.650900  0.051800    0.679200  0.694800  0.954500    1.121000  1.465600  0.492600    0.183400  -0.040800  -0.140900  
-2.632700  -2.306400  -0.651600    0.546800  0.340100  -1.102200    -0.141300  -0.148600  -0.983300    0.277700  0.826100  0.289900    -0.797800  -0.287800  0.423800    -0.461200  -0.698800  -1.238000    -2.301000  -3.376500  -0.100200    1.907800  1.074200  1.275200  

==================================Matrix Transpose==================================: 

Complete Keys transpose matrix: 

           Keys_t1                             Keys_t2                              Keys_t3                           Keys_t4                            Keys_t5                             Keys_t6                            Keys_t7                            Keys_t8 
0.400500  0.127100  0.699000      -0.270900  0.895100  0.401100      -0.210700  0.976900  0.618500      0.220500  0.527000  0.939300      0.970100  0.301400  0.617100      -0.972200  -0.504500  0.282900      1.952800  0.521800  -2.409400      -0.233300  -0.326000  -2.228900  
0.484200  0.880000  1.324200      -0.229800  -0.297500  -0.892100      0.264800  -0.269600  -0.445600      -0.538400  0.007200  -0.328600      0.167500  0.529600  0.528000      1.012900  0.044300  -0.238000      -1.539700  -0.702900  2.149300      -0.028900  -0.088300  -0.190600  
-0.799300  -0.091000  -0.345200      -0.348500  0.793900  -0.311500      -0.518100  0.928300  -0.171500      -1.000200  0.903100  1.066900      -0.185500  0.129000  0.676200      -0.628600  0.352400  -0.699100      0.883700  1.005500  -2.592900      -0.351100  -0.345000  -0.686100  

==================================Score Matrix==================================: 

Complete Score matrix: 

            score_m_h1                        score_m_h2                        score_m_h3                       score_m_h4                       score_m_h5                       score_m_h6                       score_m_h7                       score_m_h8 
-0.464940  -1.560848  -2.896450    -0.012839  0.059433  0.141468    0.102916  -0.630442  0.053603    1.961706  -1.622642  -1.768394    -1.216302  -1.104465  -2.126438    -0.622012  0.554439  -0.350212    -1.628431  -1.841033  4.368845    0.079002  0.087068  0.155438  
-1.141859  -1.179015  -2.462693    -0.146456  -0.078106  -0.439891    0.089051  0.000507  -0.237210    -0.898584  0.621761  0.576967    -0.976494  -0.604562  -0.854350    -0.556554  0.024489  -0.640508    0.367815  0.050077  -0.828186    0.007862  -0.007575  -0.304332  
-1.650331  -2.304953  -4.669460    0.157834  -0.486776  0.259254    0.499870  -1.010771  0.147458    -0.673497  0.414105  0.298681    -0.900767  -0.338206  -0.357707    0.518771  -0.234553  0.901327    0.616858  1.071929  -1.453273    -0.923857  -1.156739  -5.331952  

==================================Score Matrix/sqrt(emb size)==================================: 

Complete Score matrix: 

            score_m_h1                        score_m_h2                        score_m_h3                       score_m_h4                       score_m_h5                       score_m_h6                       score_m_h7                       score_m_h8 
-0.094906  -0.318607  -0.591235    -0.002621  0.012132  0.028877    0.021008  -0.128688  0.010942    0.400432  -0.331220  -0.360972    -0.248277  -0.225448  -0.434057    -0.126968  0.113174  -0.071487    -0.332402  -0.375799  0.891787    0.016126  0.017773  0.031729  
-0.233081  -0.240665  -0.502695    -0.029895  -0.015943  -0.089792    0.018178  0.000103  -0.048420    -0.183423  0.126916  0.117773    -0.199326  -0.123406  -0.174393    -0.113606  0.004999  -0.130743    0.075080  0.010222  -0.169053    0.001605  -0.001546  -0.062122  
-0.336872  -0.470496  -0.953150    0.032218  -0.099363  0.052920    0.102036  -0.206323  0.030100    -0.137477  0.084529  0.060968    -0.183868  -0.069036  -0.073017    0.105894  -0.047878  0.183983    0.125916  0.218807  -0.296648    -0.188581  -0.236118  -1.088380  

==================================Sum of Rows exp(score matrix)==================================: 

Head 1: 2.190263 2.183092 1.724217 
Head 2: 3.038886 2.868851 2.992502 
Head 3: 2.911479 2.971180 2.951551 
Head 4: 2.907514 3.092727 3.022624 
Head 5: 2.226178 2.543154 2.694924 
Head 6: 2.931598 2.775064 3.266949 
Head 7: 3.843423 2.932709 3.122082 
Head 8: 3.066426 2.939830 1.954582 

==================================Attention Matrices==================================: 

Complete Attention matrix: 

     Attention_matrix_h1             Attention_matrix_h2            Attention_matrix_h3              Attention_matrix_h4            Attention_matrix_h5             Attention_matrix_h6              Attention_matrix_h7              Attention_matrix_h8 
0.415228  0.331997  0.252775    0.328207  0.333084  0.338709    0.350760  0.301993  0.347247    0.513314  0.246962  0.239723    0.350441  0.358533  0.291026    0.300438  0.381985  0.317577    0.186604  0.178679  0.634717    0.331414  0.331960  0.336626  
0.362829  0.360088  0.277083    0.338305  0.343058  0.318637    0.342740  0.336601  0.320658    0.269153  0.367094  0.363753    0.322152  0.347562  0.330285    0.321654  0.362158  0.316188    0.367568  0.344485  0.287947    0.340702  0.339630  0.319668  
0.414101  0.362305  0.223594    0.345110  0.302561  0.352329    0.375200  0.275642  0.349158    0.288344  0.360020  0.351637    0.308745  0.346315  0.344939    0.340288  0.291786  0.367926    0.363279  0.398641  0.238080    0.423688  0.404019  0.172293  

 Attention Matrix has an error of: 0.000001
The attention matrix is correct!

==================================Concatenation Output Matrices==================================: 

Concatenated Output matrix: 

           Output1                          Output2                         Output3                          Output4                           Output5                           Output6                           Output7                           Output8 
0.018806  0.749480  -1.062778    0.786133  -0.203319  0.594100    0.518506  0.086559  0.160654    0.866311  -0.063921  0.433161    -0.059330  0.593325  -0.557971    -0.300603  -0.013503  -0.282813    -0.449071  0.126557  -1.106564    -0.208039  -0.529359  0.202291  
-0.053446  0.775856  -1.104880    0.757739  -0.193663  0.577421    0.486162  0.098955  0.157173    0.293716  0.104720  0.285009    -0.113605  0.600927  -0.572255    -0.309967  -0.004361  -0.308260    0.488849  -0.060080  0.237103    -0.186225  -0.512622  0.197406  
0.006238  0.733396  -1.046224    0.810674  -0.223277  0.606073    0.522480  0.079679  0.147614    0.337415  0.091601  0.295698    -0.137871  0.603790  -0.578863    -0.194626  0.005527  -0.310694    0.550866  -0.086482  0.399663    0.003423  -0.366973  0.154771  

==================================Output Linear Layer [OUTPUT MULTI-HEAD ATTENTION LAYER]==================================: 

           Output1L                        Output2L                       Output3L                      Output4L                       Output5L                       Output6L                         Output7L                       Output8L 
-0.361192  -0.071993  -0.070969  0.759290  0.018085  -0.405594  0.149709  0.392171  0.761205  0.208169  -0.257199  0.286547  0.224668  -0.039172  -0.213200  0.350192  0.171585  -0.338850  -0.267510  -0.019030  0.301992  -0.452954  0.154684  -0.028187  
-0.300937  0.109654  0.310536  0.659209  -0.042600  -0.419245  -0.290296  0.404838  0.307905  0.579215  0.072163  -0.094298  0.209541  -0.232000  0.033051  0.080012  0.355229  -0.130094  0.012281  -0.171167  0.167561  -0.293013  0.088986  -0.102790  
-0.321936  0.135965  0.313231  0.690895  -0.012063  -0.427897  -0.335362  0.430892  0.312470  0.594299  0.086335  -0.108628  0.212715  -0.274215  0.087086  0.002554  0.374023  -0.133469  0.051242  -0.212338  0.170107  -0.230130  0.053480  -0.138447  

Output Linear Layer has an error of: -0.000007
The Attention Layer is Correct!

==================================Output linear layer + Q==================================: 

              Att+Q                        Att+Q                        Att+Q                           Att+Q                          Att+Q                           Att+Q                          Att+Q                          Att+Q
-1.941592  -1.743893  -1.293969  1.070190  0.086585  -0.655594  -0.357991  -0.299729  0.415405  -0.017931  -0.910299  -1.373053  -1.058832  -1.118872  -1.343500  -0.470208  -0.898915  0.194550  0.470690  1.188670  -1.067808  -0.440054  -0.023716  -0.247087  
-1.627037  -1.031046  0.383636  0.745409  0.488100  -0.415945  -0.223596  0.991038  0.408505  0.703315  0.656163  0.517102  -0.674759  -0.882900  0.084851  0.759212  1.050029  0.824406  1.133281  1.294433  0.660161  -0.109613  0.048186  -0.243690  
-2.954637  -2.170435  -0.338369  1.237695  0.328037  -1.530097  -0.476662  0.282292  -0.670830  0.871999  0.912435  0.181272  -0.585085  -0.562015  0.510886  -0.458646  -0.324777  -1.371469  -2.249758  -3.588838  0.069907  1.677670  1.127680  1.136753  

==================================[OUTPUT NORMALIZATION LAYER]==================================: 

            Norm1                       Norm2                      Norm3                            Norm4                          Norm5                          Norm6                          Norm7                    Norm8     
1.577000  -0.306700  0.280100  0.457600  -0.757500  -0.617800  0.587300  -0.080900  0.921400  1.540500  -1.657600  -0.495800  1.688800  0.019200  -0.592500  0.300100  0.197200  1.261000  -0.013600  -0.578200  -2.831000  -0.365700  -0.441500  -0.091500  
0.738000  1.210100  1.653200  0.438200  0.650500  -1.737300  -0.728400  1.349400  -0.976400  -0.337500  1.169800  -0.702600  0.889600  0.652500  0.623100  -1.599800  0.616300  0.105300  -1.734000  0.061700  -1.559500  -0.286400  -0.512600  0.016700  
1.191100  0.710400  0.881200  0.557000  -0.984100  -1.334700  0.186400  -0.288800  -0.612300  -0.316000  0.329800  -1.057400  -0.067100  0.450400  -0.053800  0.741800  -0.526400  -0.510700  1.233100  -0.710200  2.852500  -1.887900  -1.129800  0.345400  

==================================Feed Forward Layer==================================: 

Output Linear Layer     
-0.560894  0.092110  1.228718  0.723924  -0.082849  -0.328170  1.349561  -0.186472  0.603101  -0.224586  0.025603  -0.317185  0.234475  -0.529563  0.412378  0.210089  0.411350  0.129862  -0.132028  0.238095  -0.823385  -0.278364  -1.051539  -0.159753  -0.790618  0.076255  0.757515  1.076724  0.287808  0.328561  0.999641  0.584426  -0.169254  1.786725  0.139052  0.261412  -0.064089  -0.379112  0.126007  0.120201  1.457949  0.047203  0.016200  0.551726  -0.253524  0.462693  -1.103629  -0.030725  0.134418  -0.553767  -0.141905  -0.132762  0.075915  1.261294  0.343833  0.830537  0.175903  0.604353  -0.426500  0.707053  -0.747529  0.178068  -0.200276  0.797338  -0.466181  0.906302  -0.268250  -0.190227  -0.742671  -0.402415  0.291447  0.686362  -0.107200  0.195500  -0.173900  -0.043000  0.129900  0.169100  -0.157200  0.012900  0.196400  0.163900  0.072800  0.138500  0.091700  0.170100  -0.109500  0.172100  0.195900  -0.010000  0.093100  -0.137300  0.035000  -0.048200  -0.123200  0.100100  
-0.923218  -0.055445  0.659515  1.273824  0.588208  0.176461  0.900941  0.686226  0.011146  1.522525  0.227452  0.295012  -0.103989  -0.631912  -0.051493  0.250601  1.434049  0.080303  0.199900  0.821826  -0.232424  0.169693  -1.066429  0.267075  0.060018  -0.636367  -0.038305  -0.199762  -0.038985  1.323294  0.619233  1.041937  0.016503  0.519153  -0.211900  0.546953  -0.674629  0.287868  -0.006476  0.803538  -0.321381  0.999502  -0.175650  -0.281927  -0.628071  -0.220915  0.411247  0.640662  0.160400  0.160900  -0.041400  -0.047500  -0.040300  -0.069700  -0.177000  -0.178800  0.024900  0.164800  -0.155400  -0.026600  0.111500  -0.039300  -0.176400  -0.000300  0.018800  -0.027200  -0.109600  -0.011200  0.002200  -0.057700  -0.176000  -0.100600  -0.107200  0.195500  -0.173900  -0.043000  0.129900  0.169100  -0.157200  0.012900  0.196400  0.163900  0.072800  0.138500  0.091700  0.170100  -0.109500  0.172100  0.195900  -0.010000  0.093100  -0.137300  0.035000  -0.048200  -0.123200  0.100100  
-0.072582  -0.768067  -0.136305  -0.002662  0.261415  1.171194  0.520533  1.143737  0.196903  0.254953  -0.123500  0.580553  -0.714529  0.035068  -0.183976  0.933938  -0.345281  1.032602  0.008051  -0.011827  -0.606971  -0.513915  0.448447  0.938462  0.086000  0.078300  0.062200  -0.114500  -0.155200  -0.007700  0.098400  0.032600  -0.134500  0.079600  0.059200  -0.186700  0.184400  0.070500  0.017400  0.005900  0.163600  0.066000  -0.017000  -0.102900  0.116800  0.123800  -0.056200  -0.146300  0.160400  0.160900  -0.041400  -0.047500  -0.040300  -0.069700  -0.177000  -0.178800  0.024900  0.164800  -0.155400  -0.026600  0.111500  -0.039300  -0.176400  -0.000300  0.018800  -0.027200  -0.109600  -0.011200  0.002200  -0.057700  -0.176000  -0.100600  -0.107200  0.195500  -0.173900  -0.043000  0.129900  0.169100  -0.157200  0.012900  0.196400  0.163900  0.072800  0.138500  0.091700  0.170100  -0.109500  0.172100  0.195900  -0.010000  0.093100  -0.137300  0.035000  -0.048200  -0.123200  0.100100  

==================================[OUTPUT NORMALIZATION LAYER]==================================: 

            Norm1                       Norm2                      Norm3                            Norm4                          Norm5                          Norm6                          Norm7                    Norm8     
1.553000  -0.431600  0.168400  0.505100  -0.230200  -0.206500  0.246100  0.097900  0.320200  1.117800  -2.347800  -0.207800  1.925700  0.042400  -0.312100  0.201100  -0.191800  1.226300  0.252700  -0.040500  -2.893900  -0.246400  -0.324600  -0.223300  
0.893400  1.119900  1.366700  0.415700  0.713900  -1.131100  -0.986500  1.441300  -0.649300  -0.659300  0.677800  -1.056400  1.304700  1.028800  0.710100  -1.153400  0.314100  0.098700  -1.869100  0.367300  -1.869900  -0.218600  -0.871200  0.012300  
1.186000  0.394700  0.977500  0.409000  -1.277600  -0.925400  0.030600  -0.032600  -0.347200  -0.363400  -0.014900  -1.239200  0.100900  0.967500  -0.131800  0.679900  -0.674900  -0.566500  1.517000  -0.623400  2.674200  -1.745800  -1.348600  0.354100  
INFO: [SIM 1] CSim done with 0 errors.
INFO: [SIM 3] *************** CSIM finish ***************
